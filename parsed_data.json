[
    {
        "path": "C:/Users/Simxyz/Desktop/DataScienceCarreer/4.ItConsultingGiGroup/CorsoPythonwGithub/SimoneVerrengia_DepositoCorsoPython/MentallyStabilityOfThePerson-Prediction\\README.md",
        "name": "README.md",
        "content": "\n\n# Mentally Stability Of The Person-Predication\n\n## Introduzione\n\nQuesto progetto affronta il tema critico della salute mentale attraverso l'analisi di un dataset, con l'obiettivo di sviluppare un modello di Machine Learning capace di predire la probabilit\u00e0 di uno stato depressivo. Implementa una pipeline completa che include fasi di preprocessing avanzato, addestramento di diversi modelli di classificazione, valutazione delle performance e una semplice interfaccia a menu per l'interazione.\n\n## Funzionalit\u00e0 Principali\n\nIl progetto offre le seguenti funzionalit\u00e0:\n\n* **Preprocessing Dati Avanzato:**\n    * Gestione sofisticata dei valori mancanti tramite tecniche di imputazione (mediana, moda) con logiche differenziate per sottogruppi specifici (Studenti vs Working Professionals).\n    * Normalizzazione e standardizzazione di dati testuali e categorici non uniformi (es. durate del sonno, titoli di studio).\n    * Codifica di variabili categoriche (binarie, ordinali, nominali) utilizzando mappature personalizzate, OrdinalEncoder e LabelEncoder.\n    * Raggruppamento intelligente di categorie con alta cardinalit\u00e0 (es. Professioni, Citt\u00e0) in gruppi pi\u00f9 gestibili (Professional Group, Region, Degree Group).\n    * Selezione automatica delle feature basata su test statistici (VIF e p-value) per ridurre la multicollinearit\u00e0 e migliorare la stabilit\u00e0 del modello (applicata sul training set prima dello split).\n* **Pipeline di Addestramento Modelli:**\n    * Addestramento di modelli di classificazione robusti come Logistic Regression e XGBoost.\n    * Strategie mirate per gestire il potenziale sbilanciamento delle classi nel dataset (stato depressivo presente vs assente), inclusi `class_weight`, `scale_pos_weight` e l'applicazione di SMOTE tramite `imblearn.pipeline`.\n    * Ottimizzazione degli iperparametri del modello XGBoost utilizzando GridSearchCV con cross-validation (5 fold) focalizzata sull'ottimizzazione del F1-Score.\n* **Valutazione e Confronto Performance:**\n    * Calcolo e visualizzazione delle metriche di valutazione standard: Accuracy, Precision, Recall, e F1-Score.\n    * Generazione e visualizzazione comparativa delle Matrici di Confusione per una comprensione approfondita delle performance dei modelli.\n* **Predizione su Nuovi Dati:**\n    * Applicazione del preprocessing identico a quello del training set per garantire coerenza.\n    * Caricamento del modello addestrato ottimizzato (`best_xgb_clf_smote.pkl`).\n    * Generazione di previsioni sul dataset di test e creazione di un file `submission.csv` nel formato standard ID/Predizione.\n    * Possibilit\u00e0 di ottenere una predizione per un singolo individuo inserendo i dati manualmente (richiede script dedicato).\n* **Interfaccia Utente Interattiva:**\n    * Menu testuale semplice per lanciare le varie fasi della pipeline (Addestramento, Test, Predizione Singola).\n    * Integrazione con uno script esterno per visualizzazioni aggiuntive (richiede script dedicato).\n\n## Struttura del Progetto\n\nLa struttura delle cartelle e dei file \u00e8 organizzata come segue:\n\n```\n.\n\u251c\u2500\u2500 Mentally/\n\u2502   \u251c\u2500\u2500 train.csv                       # Dataset di training originale (input)\n\u2502   \u251c\u2500\u2500 test.csv                        # Dataset di test originale (input)\n\u2502   \u251c\u2500\u2500 submission.csv                  # Output: File CSV con le previsioni sul test set\n\u2502   \u251c\u2500\u2500 cleaned_train.csv               # Output: Training set dopo preprocessing\n\u2502   \u251c\u2500\u2500 cleaned_test.csv                # Output: Test set dopo preprocessing\n\u2502   \u251c\u2500\u2500 person_test.csv                 # Output: File CSV temporaneo per input manuale singolo\n\u2502   \u251c\u2500\u2500 best_xgb_clf_smote.pkl          # Output: Modello XGBoost ottimizzato addestrato\n\u2502   \u251c\u2500\u2500 logistic_regression_smote.pkl   # Output: Modello Logistic Regression addestrato\n\u2502   \u2514\u2500\u2500 xgb_clf_default_smote.pkl       # Output: Modello XGBoost di base addestrato\n\u251c\u2500\u2500 main.py                             # Script principale con menu\n\u251c\u2500\u2500 mainTrain.py                        # Modulo: Addestramento, valutazione, salvataggio\n\u251c\u2500\u2500 mainTest.py                         # Modulo: Predizione su test set e submission\n\u251c\u2500\u2500 preprocessing.py                    # Modulo: Funzioni centralizzate di pulizia e trasformazione dati\n\u251c\u2500\u2500 insertUtente.py                     # Modulo: Gestione input manuale utente (CODICE NON FORNITO)\n\u2514\u2500\u2500 grafici.py                          # Modulo: Funzioni per visualizzazioni aggiuntive (CODICE NON FORNITO)\n```\n\n## Requisiti di Sistema e Installazione\n\n### Requisiti\n\nAssicurati di avere **Python 3.6+** installato. Le librerie Python necessarie sono:\n\n* `pandas`\n* `numpy`\n* `scikit-learn`\n* `imbalanced-learn`\n* `xgboost`\n* `statsmodels`\n* `seaborn`\n* `matplotlib`\n* `joblib`\n\nPuoi installarle tutte tramite pip:\n\n```bash\npip install pandas numpy scikit-learn imbalanced-learn xgboost statsmodels seaborn matplotlib joblib\n```\n\nIl modulo `re` \u00e8 una libreria standard di Python e non richiede installazione aggiuntiva.\n\n### Installazione\n\n1.  Clona il repository:\n    ```bash\n    git clone <URL_DEL_TUO_REPOSITORY>\n    cd <nome_cartella_progetto>\n    ```\n2.  Crea la cartella necessaria per i dati e gli output:\n    ```bash\n    mkdir Mentally\n    ```\n3.  Posiziona i file `train.csv` e `test.csv` (ottenuti dal dataset) all'interno della cartella `Mentally/`.\n\n## Utilizzo del Programma\n\nPer avviare l'applicazione interattiva, esegui lo script principale dal terminale nella directory radice del progetto:\n\n```bash\npython main.py\n```\n\nTi verr\u00e0 presentato un menu:\n\n```\nBenvenuto nel menu principale!\n1. Analisi, preprocessing, addestramento e previsione su file CSV di training\n2. Analisi, preprocessing, previsione su test e generazione submission\n3. Predizione dello stato depressivo di una persona (inserimento manuale)\n4. Visualizza grafici\n5. Esci\n```\n\nSeleziona l'opzione desiderata digitando il numero corrispondente.\n\n* **Opzione 1 (Addestramento):** Questa \u00e8 la fase iniziale. Carica `train.csv`, pulisce i dati (`preprocessing.py`), seleziona le feature, addestra e ottimizza i modelli, valuta i risultati (con output a console e grafici delle matrici/metriche) e salva i modelli addestrati (`.pkl`) e il dataset pulito in `Mentally/`. **\u00c8 indispensabile eseguire questa opzione almeno una volta prima di procedere con le opzioni 2 e 3, poich\u00e9 queste ultime dipendono dai modelli salvati.**\n* **Opzione 2 (Test e Submission):** Carica `test.csv`, applica le stesse trasformazioni del training, carica il miglior modello addestrato (`best_xgb_clf_smote.pkl`) ed effettua le predizioni per generare il file `Mentally/submission.csv`.\n* **Opzione 3 (Predizione Singola):** Permette di inserire manualmente i dati di un individuo per ottenere una predizione in tempo reale. Richiama le logiche di preprocessing per un singolo record (`preprocess_person_test` in `preprocessing.py`) e utilizza il modello salvato. **Nota: Richiede la presenza del file `insertUtente.py`.**\n* **Opzione 4 (Visualizzazioni):** Accede a un sottomenu o a funzionalit\u00e0 grafiche definite nello script `grafici.py`. **Nota: Richiede la presenza del file `grafici.py`.**\n* **Opzione 5 (Esci):** Termina l'esecuzione del programma.\n\n## Descrizione Dettagliata dei Moduli\n\n### `main.py`\n\nAgisce da orchestratore. Contiene la funzione `menu()` che gestisce il loop principale, presenta le opzioni all'utente, cattura l'input e invoca le funzioni appropriate dagli altri moduli (`mainTrain.py`, `mainTest.py`, `insertUtente.py`, `grafici.py`) basandosi sulla scelta effettuata.\n\n### `mainTrain.py`\n\n\u00c8 il cuore del processo di addestramento. La funzione `train()` al suo interno si occupa di:\n1.  Caricare i dati di training.\n2.  Invocare `preprocess_train` per pulire e trasformare il DataFrame.\n3.  Separare feature (`X`) e target (`y`).\n4.  Applicare la selezione feature (`elimina_variabili_vif_pvalue`).\n5.  Suddividere il dataset (gi\u00e0 con feature selezionate) in set di addestramento e test *interni* (`X_train`, `X_test`, `y_train`, `y_test`) in modo stratificato.\n6.  Inizializzare i vari modelli (Logistic Regression con bilanciamento, XGBoost con pesi di posizione).\n7.  Costruire una pipeline `imblearn` che combina SMOTE con XGBoost.\n8.  Definire e eseguire una ricerca a griglia (`GridSearchCV`) sulla pipeline SMOTE+XGBoost per trovare i migliori iperparametri, ottimizzando l'F1-Score.\n9.  Addestrare tutti i modelli definiti (LogReg, XGBoost base, XGBoost ottimizzato dalla Grid Search) sui dati di addestramento interni (`X_train`, `y_train`).\n10. Salvare i modelli addestrati (in formato `.pkl`) nella cartella `Mentally/`.\n11. Generare predizioni sui dati di test interni (`X_test`) per valutare i modelli.\n12. Chiamare le funzioni di visualizzazione (`plot_combined_confusion_matrices`, `plot_metrics_comparison`) per mostrare graficamente le performance dei modelli a confronto.\n\nLe funzioni `plot_combined_confusion_matrices` e `plot_metrics_comparison` sono definite in questo modulo per facilitare la visualizzazione dei risultati della valutazione.\n\n### `mainTest.py`\n\nContiene la funzione `test()` responsabile della fase di predizione finale sul test set originale:\n1.  Carica il dataset `Mentally/test.csv`.\n2.  Applica le trasformazioni chiamando la funzione `preprocess_test` da `preprocessing.py`. Questa funzione restituisce il DataFrame pulito e gli ID originali, garantendo che le predizioni possano essere associate correttamente.\n3.  Carica il modello ottimizzato salvato durante l'addestramento (`best_xgb_clf_smote.pkl`).\n4.  Utilizza il modello caricato per generare le predizioni sullo stato depressivo (`y_pred_class`) per ogni riga del test set pulito.\n5.  Crea un DataFrame `submission` combinando gli ID originali e le predizioni.\n6.  Salva il DataFrame `submission` in formato CSV (`Mentally/submission.csv`) senza l'indice.\n\n### `preprocessing.py`\n\nQuesto modulo \u00e8 fondamentale e centralizza tutta la logica di pulizia e trasformazione dei dati per garantire coerenza tra training, test e input manuale. Contiene le seguenti funzioni:\n\n* `map_sleep_duration(duration_str)`: Una funzione ausiliaria che utilizza espressioni regolari per interpretare e convertire stringhe eterogenee che descrivono la durata del sonno in valori numerici (ore).\n* `elimina_variabili_vif_pvalue(X, y, vif_threshold, pvalue_threshold)`: Implementa un algoritmo di selezione delle feature backward-stepwise. Itera rimuovendo le feature che contemporaneamente superano una soglia di VIF (indicando alta multicollinearit\u00e0) e una soglia di p-value (indicando bassa significativit\u00e0 statistica in un modello lineare semplice), finch\u00e9 non vengono soddisfatte le condizioni.\n* `preprocess_train(df)`: Implementa l'intera pipeline di preprocessing per il dataset di training. Include la gestione dei NaN con logiche basate sul ruolo (Student/Professional), imputazione con mediana/moda, codifica binaria (Genere, Stato Lavorativo/Studentesco, Suicidal Thoughts, Family History), applicazione di `map_sleep_duration`, pulizia e codifica ordinale del Grado di Istruzione (`Degree_Group_Encoded`), pulizia, validazione e codifica ordinale delle Abitudini Alimentari (`Dietary Habits`), raggruppamento e codifica nominale della Professione (`Professional_Group_Encoded`) e della Citt\u00e0/Regione (`Region_Encoded`). Infine, rimuove le colonne originali e quelle intermedie non pi\u00f9 necessarie e salva il DataFrame pulito in `cleaned_train.csv`. **Nota:** Il codice rimuove la colonna `Have you ever had suicidal thoughts ?` (rinominata in `SuicidalThoughts`) alla fine del preprocessing.\n* `preprocess_test(df)`: Implementa una pipeline di preprocessing identica a `preprocess_train` ma specifica per il dataset di test. \u00c8 cruciale che le trasformazioni (mappature, imputazioni) siano basate *solo* sui dati del training set originale per evitare data leakage. Questa funzione restituisce il DataFrame pulito e gli ID originali del test set. Salva il DataFrame pulito in `cleaned_test.csv`.\n* `preprocess_person_test(df)`: Una versione della pipeline di preprocessing ottimizzata per gestire un singolo record di input (presumibilmente da `insertUtente.py`). Applica le stesse trasformazioni di imputazione e codifica per rendere il formato del singolo record compatibile con quello dei dati su cui \u00e8 stato addestrato il modello. Salva il record processato in `person_test.csv` (probabilmente temporaneo).\n\n### `insertUtente.py` \n\nBasato sull'importazione nel `main.py`, questo script \u00e8 atteso per contenere la funzione `insert_data()`. Il suo scopo \u00e8 di:\n1.  Interagire con l'utente tramite input da console (o altra GUI) per raccogliere i dati di una singola persona (et\u00e0, genere, professione, ecc.).\n2.  Organizzare questi dati in un formato compatibile con le funzioni di preprocessing (tipicamente un DataFrame pandas con una singola riga).\n3.  Chiamare la funzione `preprocess_person_test` da `preprocessing.py` per pulire e trasformare l'input dell'utente.\n4.  Caricare il modello addestrato (es. `best_xgb_clf_smote.pkl`).\n5.  Effettuare una predizione sullo stato depressivo per l'input pre-processato.\n6.  Comunicare la predizione risultante all'utente.\n\n### `grafici.py` \n\nBasato sull'importazione nel `main.py`, questo script \u00e8 atteso per contenere la funzione `menu_visualizzazioni()` e potenzialmente altre funzioni di supporto per la generazione di grafici. Il suo scopo \u00e8 di fornire visualizzazioni aggiuntive rispetto a quelle incluse in `mainTrain.py`, che potrebbero includere:\n1.  Visualizzazioni esplorative del dataset originale (distribuzioni di feature, relazioni tra variabili).\n2.  Visualizzazioni relative ai risultati o alle performance del modello (oltre matrici di confusione e metriche riassuntive).\nUtilizzerebbe tipicamente le librerie `matplotlib` e `seaborn`.\n\n*Autore: Giacomo Visciotti-Simone Verrengia-Giuseppe Pio del Vecchio-Liliana Gilca*\n\n"
    },
    {
        "path": "C:/Users/Simxyz/Desktop/DataScienceCarreer/4.ItConsultingGiGroup/CorsoPythonwGithub/SimoneVerrengia_DepositoCorsoPython/MentallyStabilityOfThePerson-Prediction\\Mentally\\grafici.py",
        "name": "grafici.py",
        "content": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Distribuzione della Frequenza del Genere\ndef plot_gender_distribution(train):\n    gender_labels = {0: 'Femmina', 1: 'Maschio'}\n    gender_counts = train['Gender'].value_counts()\n    gender_counts_labeled = gender_counts.rename(index=gender_labels)\n\n    plt.figure(figsize=(8, 6))\n    ax = sns.barplot(x=gender_counts_labeled.index, y=gender_counts_labeled.values, palette='viridis')\n    plt.title('Distribuzione della Frequenza del Genere', fontsize=16)\n    plt.xlabel('Genere', fontsize=12)\n    plt.ylabel('Frequenza (Numero di Individui)', fontsize=12)\n\n    for container in ax.containers:\n        ax.bar_label(container, fmt='%.0f')\n\n    plt.tight_layout()\n    plt.show()\n\n# Distribuzione della Frequenza: Studente vs Professionista\ndef plot_status_distribution(train):\n    status_counts = train['Working Professional or Student'].value_counts()\n    status_labels = {0: 'Studente', 1: 'Professionista'}\n    status_counts_labeled = status_counts.rename(index=status_labels)\n\n    plt.figure(figsize=(8, 6))\n    ax = sns.barplot(x=status_counts_labeled.index, y=status_counts_labeled.values, palette='viridis')\n    plt.title('Distribuzione della Frequenza: Studente vs Professionista', fontsize=16)\n    plt.xlabel('Status', fontsize=12)\n    plt.ylabel('Frequenza (Numero di Individui)', fontsize=12)\n\n    for container in ax.containers:\n        ax.bar_label(container, fmt='%.0f')\n\n    plt.tight_layout()\n    plt.show()\n\n# Frequenza della Depressione per Status \ndef plot_depression_by_status(train):\n    # Mappiamo le etichette per chiarezza\n    status_labels = {0: 'Studente', 1: 'Professionista'}\n    depression_labels = {0: 'No Depressione', 1: 'S\u00ec Depressione'}\n\n    # Creiamo una tabella di contingenza\n    grouped = train.groupby(['Working Professional or Student', 'Depression']).size().unstack()\n\n    # Rinominiamo righe e colonne\n    grouped.index = grouped.index.map(status_labels)\n    grouped.columns = [depression_labels[col] for col in grouped.columns]\n\n    # Plot a barre\n    ax = grouped.plot(kind='bar', figsize=(8, 6), color=['#1f77b4', '#2ca02c'])\n    plt.title('Frequenza della Depressione per Status (Studente vs Professionista)', fontsize=16)\n    plt.xlabel('Status', fontsize=12)\n    plt.ylabel('Numero di Individui', fontsize=12)\n    plt.xticks(rotation=0)\n\n    for container in ax.containers:\n        ax.bar_label(container, fmt='%.0f')\n\n    plt.legend(title='Condizione di Depressione')\n    plt.tight_layout()\n    plt.show()\n\n# Frequenza di Risposte su Pensieri Suicidi Precedenti\ndef plot_suicidal_thoughts_distribution(suicidal_thoughts_counts):\n    suicidal_thoughts_labels = {0: 'No', 1: 'S\u00ec'}\n    suicidal_thoughts_counts_labeled = suicidal_thoughts_counts.rename(index=suicidal_thoughts_labels)\n\n    plt.figure(figsize=(8, 6))\n    ax = sns.barplot(x=suicidal_thoughts_counts_labeled.index, y=suicidal_thoughts_counts_labeled.values, palette='viridis')\n    plt.title('Frequenza di Risposte su Pensieri Suicidi Precedenti', fontsize=16)\n    plt.xlabel('Risposta', fontsize=12)\n    plt.ylabel('Frequenza (Numero di Individui)', fontsize=12)\n\n    for container in ax.containers:\n        ax.bar_label(container, fmt='%.0f')\n\n    plt.tight_layout()\n    plt.show()\n\n# Frequenza della Condizione di Depressione \ndef plot_depression_distribution(train):\n    depression_counts = train['Depression'].value_counts()\n    depression_labels = {0: 'No', 1: 'S\u00ec'}\n    depression_counts_labeled = depression_counts.rename(index=depression_labels)\n\n    plt.figure(figsize=(7, 5))\n    ax = sns.barplot(x=depression_counts_labeled.index, y=depression_counts_labeled.values, palette='viridis')\n    plt.title('Frequenza della Condizione di Depressione', fontsize=16)\n    plt.xlabel('Presenza di Depressione', fontsize=12)\n    plt.ylabel('Frequenza (Numero di Individui)', fontsize=12)\n\n    for container in ax.containers:\n        ax.bar_label(container, fmt='%.0f')\n\n    plt.tight_layout()\n    plt.show()\n\n# Frequenza della Depressione per Genere\ndef plot_depression_by_gender(train):\n    gender_labels = {0: 'Femmina', 1: 'Maschio'}\n    depression_labels = {0: 'No Depressione', 1: 'S\u00ec Depressione'}\n\n    plt.figure(figsize=(10, 6))\n    ax = sns.countplot(data=train, x='Gender', hue='Depression', palette='viridis')\n    plt.title('Frequenza della Depressione per Genere', fontsize=16)\n    plt.xlabel('Genere', fontsize=12)\n    plt.ylabel('Frequenza (Numero di Individui)', fontsize=12)\n    plt.xticks(ticks=[0, 1], labels=[gender_labels[0], gender_labels[1]], rotation=0)\n\n    handles, labels = ax.get_legend_handles_labels()\n    new_labels = [depression_labels[int(lbl)] for lbl in labels]\n    ax.legend(handles, new_labels, title='Condizione Depressione')\n\n    for container in ax.containers:\n        for p in container.patches:\n            height = p.get_height()\n            if height > 0:\n                ax.text(p.get_x() + p.get_width() / 2., height + 50,\n                        f'{int(height)}', ha='center', va='bottom')\n\n    plt.tight_layout()\n    plt.show()\n\n# Frequenza dei Pensieri Suicidi Precedenti in base alla Depressione\ndef plot_suicidal_thoughts_by_depression(train):\n    depression_labels = {0: 'No Depressione', 1: 'S\u00ec Depressione'}\n    suicidal_thoughts_labels = {'0': 'No (Pensieri)', '1': 'S\u00ec (Pensieri)', 0: 'No (Pensieri)', 1: 'S\u00ec (Pensieri)'}\n\n    plt.figure(figsize=(9, 6))\n    ax = sns.countplot(data=train, x='Depression', hue='Have you ever had suicidal thoughts ?', palette='pastel')\n    plt.title('Frequenza dei Pensieri Suicidi Precedenti in base alla Depressione', fontsize=16)\n    plt.xlabel('Condizione di Depressione', fontsize=12)\n    plt.ylabel('Frequenza (Numero di Individui)', fontsize=12)\n    plt.xticks(ticks=[0, 1], labels=[depression_labels[0], depression_labels[1]], rotation=0)\n\n    handles, labels = ax.get_legend_handles_labels()\n    new_legend_labels = [suicidal_thoughts_labels.get(lbl, lbl) for lbl in labels]\n    ax.legend(handles, new_legend_labels, title='Pensieri Suicidi Precedenti')\n\n    for container in ax.containers:\n        for p in container.patches:\n            height = p.get_height()\n            if height > 0:\n                ax.text(p.get_x() + p.get_width() / 2., height + 50, f'{int(height)}', ha='center', va='bottom')\n\n    plt.tight_layout()\n    plt.show()\n\n# --- Frequenza della Depressione per Regione ---\n\n# 0     Central India              0\n# 1        East India              1\n# 2  North-East India              2\n# 3  North-West India              3\n# 4             Other              4\n# 5       South India              5\n# 6      West-Gujarat              6\n# 7  West-Maharashtra              7\ndef plot_depression_by_region(train):\n    depression_labels = {0: 'No Depressione', 1: 'S\u00ec Depressione'}\n    region_codes_order_sorted = sorted(train['Region_Encoded'].unique())\n    region_labels = {\n        0: 'Central India',\n        1: 'East India',\n        2: 'North-East India',\n        3: 'North-West India',\n        4: 'Other',\n        5: 'South India',\n        6: 'West-Gujarat',\n        7: 'West-Maharashtra'\n    }\n\n    # Create the countplot\n    plt.figure(figsize=(14, 7))\n    ax = sns.countplot(\n        data=train,\n        x='Region_Encoded',\n        hue='Depression',\n        palette='viridis',\n        order=region_codes_order_sorted\n    )\n\n    # Replace numeric x-tick labels with region names\n    xtick_names = [region_labels[code] for code in region_codes_order_sorted]\n    ax.set_xticklabels(xtick_names, rotation=45, ha='right')\n\n    # Titles and labels\n    plt.title('Frequenza della Depressione per Regione', fontsize=16)\n    plt.xlabel('Regione', fontsize=12)\n    plt.ylabel('Frequenza (Numero di Individui)', fontsize=12)\n\n    # Update legend labels\n    handles, labels = ax.get_legend_handles_labels()\n    new_legend_labels = [depression_labels[int(lbl)] for lbl in labels]\n    ax.legend(handles, new_legend_labels, title='Condizione Depressione')\n\n    plt.tight_layout()\n    plt.show()\n\n# Distribuzione dello Stress Finanziario per Stato di Depressione\ndef plot_financial_stress_by_depression(train):\n    depression_labels_plot = {0: 'No Depressione', 1: 'S\u00ec Depressione'}\n\n    plt.figure(figsize=(8, 6))\n    ax = sns.boxplot(data=train, x='Depression', y='Financial Stress', palette='viridis')\n    plt.title('Distribuzione dello Stress Finanziario per Stato di Depressione', fontsize=16)\n    plt.xlabel('Condizione di Depressione', fontsize=12)\n    plt.ylabel('Stress Finanziario', fontsize=12)\n    plt.xticks(ticks=[0, 1], labels=[depression_labels_plot[0], depression_labels_plot[1]], rotation=0)\n    plt.tight_layout()\n    plt.show()\n\n# Distribuzione dell'Et\u00e0 per Stato di Depressione \ndef plot_age_distribution_by_depression(train):\n    depression_labels_plot = {0: 'No Depressione', 1: 'S\u00ec Depressione'}\n\n    plt.figure(figsize=(8, 6))\n    ax = sns.boxplot(data=train, x='Depression', y='Age', palette='viridis')\n    plt.title(\"Distribuzione dell'Et\u00e0 per Stato di Depressione\", fontsize=16)\n    plt.xlabel('Condizione di Depressione', fontsize=12)\n    plt.ylabel('Et\u00e0', fontsize=12)\n    plt.xticks(ticks=[0, 1], labels=[depression_labels_plot[0], depression_labels_plot[1]], rotation=0)\n\n    plt.tight_layout()\n    plt.show()\n   \n# Frequenza della Depressione per Gruppo di Laurea\ndef plot_depression_by_degree_group(train):\n    \"\"\"\n    Plotta la frequenza della depressione suddivisa per gruppo di laurea utilizzando le etichette testuali.\n    \"\"\"\n    depression_labels = {0: 'No Depressione', 1: 'S\u00ec Depressione'}\n    degree_order = sorted(train['Degree_Group_Encoded'].unique())\n    degree_labels = {\n        0: 'Other',\n        1: 'High School',\n        2: 'Bachelor',\n        3: 'Master',\n        4: 'Doctorate'\n    }\n\n    plt.figure(figsize=(10, 6))\n    ax = sns.countplot(\n        data=train,\n        x='Degree_Group_Encoded',\n        hue='Depression',\n        palette='viridis',\n        order=degree_order\n    )\n\n    # Sostituisci i codici dei gruppi di laurea con le etichette testuali\n    xtick_names = [degree_labels[code] for code in degree_order]\n    ax.set_xticklabels(xtick_names, rotation=0, ha='center')\n\n    # Titolo e assi\n    plt.title('Frequenza della Depressione per Gruppo di Laurea', fontsize=16)\n    plt.xlabel('Gruppo di Laurea', fontsize=12)\n    plt.ylabel('Frequenza (Numero di Individui)', fontsize=12)\n\n    # Legenda con etichette di depressione testuali\n    handles, labels = ax.get_legend_handles_labels()\n    new_legend_labels = [depression_labels[int(lbl)] for lbl in labels]\n    ax.legend(handles, new_legend_labels, title='Condizione Depressione')\n\n    # Aggiungi etichette sui bar\n    for container in ax.containers:\n        ax.bar_label(container, fmt='%.0f', label_type='edge', padding=3)\n\n    plt.tight_layout()\n    plt.show()\n\n# Frequenza della Depressione per Livello di Soddisfazione nello Studio\ndef plot_depression_by_study_satisfaction(train):\n    depression_labels = {0: 'No Depressione', 1: 'S\u00ec Depressione'}\n    satisfaction_order = sorted(train['Study Satisfaction'].unique())\n\n    plt.figure(figsize=(10, 6))\n    ax = sns.countplot(data=train, x='Study Satisfaction', hue='Depression',\n                       palette='viridis', order=satisfaction_order)\n\n    plt.title('Frequenza della Depressione per Livello di Soddisfazione nello Studio', fontsize=16)\n    plt.xlabel('Soddisfazione nello Studio (Scala)', fontsize=12)\n    plt.ylabel('Frequenza (Numero di Individui)', fontsize=12)\n    plt.xticks(rotation=0)\n\n    handles, labels = ax.get_legend_handles_labels()\n    new_legend_labels = [depression_labels[int(lbl)] for lbl in labels]\n    ax.legend(handles, new_legend_labels, title='Condizione Depressione')\n\n    for container in ax.containers:\n        ax.bar_label(container, fmt='%.0f', label_type='edge', padding=3)\n\n    plt.tight_layout()\n    plt.show()\n\n# Matrice di correlazione\ndef plot_pearson_correlation(train):\n    cols = ['Age', 'CGPA', 'Sleep Duration', 'Work/Study Hours']\n    corr_matrix = train[cols].corr(method='pearson')\n\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(corr_matrix, annot=True, cmap='viridis', fmt='.2f', square=True,\n                linewidths=0.5, linecolor='white', cbar_kws={'shrink': .75})\n    plt.title('Matrice di Correlazione di Pearson', fontsize=16)\n    plt.tight_layout()\n    plt.show()\n\ndef menu_visualizzazioni():\n    train = pd.read_csv('Mentally/cleaned_train.csv')\n\n    print(train[['Gender', 'Depression']].head())\n\n    # Calcolo della distribuzione proporzionale\n    proportions = train.groupby('Gender')['Depression'].value_counts(normalize=True).unstack()\n\n    # Moltiplica per 100 per ottenere percentuali\n    proportions_percentuali = proportions * 100\n\n    # Visualizza il risultato\n    print(proportions_percentuali)\n    while True:\n        print(\"\\n MENU VISUALIZZAZIONI\")\n        print(\"1. Distribuzione del Genere\")\n        print(\"2. Distribuzione Studente vs Professionista\")\n        print(\"3. Frequenza Pensieri Suicidi Precedenti\")\n        print(\"4. Frequenza della Condizione di Depressione\")\n        print(\"5. Depressione per Genere\")\n        print(\"6. Pensieri Suicidi in base alla Depressione\")\n        print(\"7. Depressione per Regione\")\n        print(\"8. Stress Finanziario per Stato di Depressione\")\n        print(\"9. Et\u00e0 per Stato di Depressione\")\n        print(\"10. Correlazione Pearson (Age, CGPA, Sleep, Work Hours)\")\n        print(\"11. Depressione per Gruppo di Laurea\")\n        print(\"12. Depressione per Soddisfazione nello Studio\")\n        print(\"13. Depressione per Status (Studente vs Professionista)\")\n        print(\"0. Esci\")\n\n        scelta = input(\"\\nSeleziona un'opzione (0-13): \")\n\n        if scelta == '1':\n            plot_gender_distribution(train)\n        elif scelta == '2':\n            plot_status_distribution(train)\n        elif scelta == '3':\n            suicidal_thoughts_counts = train['Have you ever had suicidal thoughts ?'].value_counts()\n            plot_suicidal_thoughts_distribution(suicidal_thoughts_counts)\n        elif scelta == '4':\n            plot_depression_distribution(train)\n        elif scelta == '5':\n            plot_depression_by_gender(train)\n        elif scelta == '6':\n            plot_suicidal_thoughts_by_depression(train)\n        elif scelta == '7':\n            plot_depression_by_region(train)\n        elif scelta == '8':\n            plot_financial_stress_by_depression(train)\n        elif scelta == '9':\n            plot_age_distribution_by_depression(train)\n        elif scelta == '10':\n            plot_pearson_correlation(train)\n        elif scelta == '11':\n            plot_depression_by_degree_group(train)\n        elif scelta == '12':\n            plot_depression_by_study_satisfaction(train)\n        elif scelta == '13':\n            plot_depression_by_status(train)\n        elif scelta == '0':\n            print(\"Uscita dal men\u00f9.\")\n            break\n        else:\n            print(\"Scelta non valida. Riprova.\")\n"
    },
    {
        "path": "C:/Users/Simxyz/Desktop/DataScienceCarreer/4.ItConsultingGiGroup/CorsoPythonwGithub/SimoneVerrengia_DepositoCorsoPython/MentallyStabilityOfThePerson-Prediction\\Mentally\\graphicGui.py",
        "name": "graphicGui.py",
        "content": "import matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# Ogni funzione restituisce un oggetto Figure\n\ndef plot_gender_distribution(df):\n    fig, ax = plt.subplots(figsize=(6, 4))\n    sns.countplot(data=df, x='Gender', hue='Gender', ax=ax, palette='viridis', legend=False)\n    ax.set_title('Distribuzione Genere')\n    ax.set_xlabel('Genere')\n    ax.set_ylabel('Conteggio')\n    plt.close(fig)\n    return fig\n\n\ndef plot_status_distribution(df):\n    fig, ax = plt.subplots(figsize=(8, 5))\n    sns.countplot(data=df, x='Working Professional or Student', hue='Working Professional or Student', ax=ax, palette='viridis', legend=False)\n    ax.set_title('Distribuzione Studente vs Professionista')\n    ax.set_xlabel('Status')\n    ax.set_ylabel('Conteggio')\n    plt.xticks(rotation=45, ha='right') \n    plt.tight_layout()\n    plt.close(fig)\n    return fig\n\n\ndef plot_suicidal_thoughts_distribution(counts):\n    fig, ax = plt.subplots(figsize=(6, 4))\n    counts.plot(kind='bar', ax=ax, color=['skyblue', 'salmon'])\n    ax.set_title('Distribuzione Pensieri Suicidi Precedenti')\n    ax.set_xlabel('Ha avuto pensieri suicidi?')\n    ax.set_ylabel('Conteggio')\n    plt.xticks(rotation=0)\n    plt.tight_layout()\n    plt.close(fig)\n    return fig\n\n\ndef plot_depression_distribution(df):\n    fig, ax = plt.subplots(figsize=(6, 4))\n    sns.countplot(data=df, x='Depression', hue='Depression', ax=ax, palette='coolwarm', legend=False)\n    ax.set_title('Distribuzione Condizione di Depressione')\n    ax.set_xlabel('Depressione (0=No, 1=S\u00ec)')\n    ax.set_ylabel('Conteggio')\n    plt.xticks([0, 1], ['No', 'S\u00ec'])\n    plt.close(fig)\n    return fig\n\n\ndef plot_depression_by_gender(df):\n    fig, ax = plt.subplots(figsize=(7, 5))\n    sns.countplot(data=df, x='Gender', hue='Depression', ax=ax, palette='coolwarm')\n    ax.set_title('Depressione per Genere')\n    ax.set_xlabel('Genere')\n    ax.set_ylabel('Conteggio')\n    \n    handles, labels = ax.get_legend_handles_labels()\n    if handles: \n        if len(handles) == 2: # Assumendo 0='No', 1='S\u00ec'\n            ax.legend(handles, ['No', 'S\u00ec'], title='Depressione')\n        else: # Altrimenti, usa le etichette generate da Seaborn\n            ax.legend(handles=handles, labels=labels, title='Depressione')\n    plt.close(fig)\n    return fig\n\n\ndef plot_suicidal_thoughts_by_depression(df):\n    fig, ax = plt.subplots(figsize=(7, 5))\n    sns.countplot(data=df, x='Have you ever had suicidal thoughts ?', hue='Depression', ax=ax, palette='coolwarm')\n    ax.set_title('Pensieri Suicidi vs Depressione')\n    ax.set_xlabel('Pensieri Suicidi Precedenti')\n    ax.set_ylabel('Conteggio')\n\n    handles, labels = ax.get_legend_handles_labels()\n    if handles:\n        if len(handles) == 2:\n            ax.legend(handles, ['No', 'S\u00ec'], title='Depressione')\n        else:\n            ax.legend(handles=handles, labels=labels, title='Depressione')\n    plt.close(fig)\n    return fig\n\n\ndef plot_depression_by_Region_Encoded(df):\n    \"\"\"\n    Incidenza di Depressione per Regione (encoded \u2192 etichetta)\n    \"\"\"\n    region_labels = {\n        0: 'North', 1: 'South', 2: 'East', 3: 'West', 4: 'Central'\n    }\n    fig, ax = plt.subplots(figsize=(10, 6))\n    if 'Region_Encoded' not in df.columns or 'Depression' not in df.columns:\n        ax.text(0.5, 0.5, \"Dati per 'Region_Encoded' o 'Depression' non disponibili\",\n                ha='center', va='center', fontsize=12)\n        ax.axis('off'); plt.close(fig); return fig\n\n    counts = (df.groupby('Region_Encoded')['Depression']\n              .value_counts(normalize=True).unstack(fill_value=0)\n              .sort_values(by=1, ascending=False).head(15)) # Assumendo che la colonna 1 sia 'S\u00ec Depressione'\n    \n    if counts.empty:\n        ax.text(0.5, 0.5, \"Nessun dato da visualizzare per le regioni.\",\n                ha='center', va='center', fontsize=12)\n        ax.axis('off'); plt.close(fig); return fig\n\n    counts.plot(kind='bar', stacked=True, ax=ax, colormap='coolwarm')\n    \n    bar_codes = counts.index.tolist()\n    tick_display_labels = [region_labels.get(c, f\"Codice {c}\") for c in bar_codes]\n\n    ax.set_xticks(np.arange(len(bar_codes)))\n    ax.set_xticklabels(tick_display_labels, rotation=45, ha='right')\n    \n    ax.set_title('Incidenza di Depressione per Regione (Top 15)', fontsize=16)\n    ax.set_xlabel('Regione', fontsize=12); ax.set_ylabel('Proporzione', fontsize=12)\n    \n    if 0 in counts.columns and 1 in counts.columns:\n         ax.legend(title='Depressione', labels=['No', 'S\u00ec']) \n    else: # Fallback generico\n        current_handles, current_labels = ax.get_legend_handles_labels()\n        if current_handles:\n            ax.legend(handles=current_handles, labels=current_labels, title='Depressione')\n\n\n    for c_idx, c in enumerate(ax.containers):\n        text_color = 'white'\n        labels_for_bar = [f'{h:.0%}' if (h := bar.get_height()) > 0.02 else '' for bar in c]\n        ax.bar_label(c, labels=labels_for_bar, label_type='center', color=text_color, fontsize=8, fontweight='bold')\n    plt.tight_layout(); plt.close(fig); return fig\n\n\ndef plot_depression_by_Degree_Group_Encoded_group(df):\n    \"\"\"\n    Frequenza della Depressione per Gruppo di Laurea\n    \"\"\"\n    degree_labels = {0: 'Other', 1: 'High School', 2: 'Bachelor', 3: 'Master', 4: 'Doctorate'}\n    depression_labels = {0: 'No Depressione', 1: 'S\u00ec Depressione'} # Usato per la legenda\n    fig, ax = plt.subplots(figsize=(10, 6))\n    if 'Degree_Group_Encoded' not in df.columns or 'Depression' not in df.columns:\n        ax.text(0.5, 0.5, \"Dati per 'Degree_Group_Encoded' non disponibili\",\n                ha='center', va='center', fontsize=12)\n        ax.axis('off'); plt.close(fig); return fig\n\n    vc = df['Degree_Group_Encoded'].value_counts()\n    top_codes = sorted(vc[vc > 10].index.tolist())\n    if not top_codes:\n        ax.text(0.5, 0.5, \"Nessun gruppo con pi\u00f9 di 10 occorrenze\",\n                ha='center', va='center', fontsize=12)\n        ax.axis('off'); plt.close(fig); return fig\n\n    sub = df[df['Degree_Group_Encoded'].isin(top_codes)]\n    sns.countplot(data=sub, x='Degree_Group_Encoded', hue='Depression',\n                  order=top_codes, palette='coolwarm', ax=ax)\n    \n    tick_display_labels = [degree_labels.get(c, f\"Codice {c}\") for c in top_codes]\n    ax.set_xticks(np.arange(len(top_codes)))\n    ax.set_xticklabels(tick_display_labels, rotation=45, ha='right')\n    \n    ax.set_title('Frequenza della Depressione per Gruppo di Laurea (pi\u00f9 comuni)', fontsize=16)\n    ax.set_xlabel('Gruppo di Laurea', fontsize=12)\n    ax.set_ylabel('Frequenza (Numero di Individui)', fontsize=12)\n    \n    handles, current_labels = ax.get_legend_handles_labels()\n    if handles: \n        try:\n            mapped_labels = [depression_labels[int(l)] for l in current_labels]\n            ax.legend(handles, mapped_labels, title='Depressione')\n        except (ValueError, KeyError): \n            if len(handles) == 2:\n                ax.legend(handles, ['No', 'S\u00ec'], title='Depressione') # Fallback generico\n            else:\n                ax.legend(handles=handles, labels=current_labels, title='Depressione')\n\n    for container in ax.containers:\n        ax.bar_label(container, fmt='%.0f', label_type='edge', padding=3)\n    plt.tight_layout(); plt.close(fig); return fig\n\n\ndef plot_financial_stress_by_depression(df):\n    fig, ax = plt.subplots(figsize=(8, 5))\n    sns.boxplot(data=df, x='Depression', y='Financial Stress', hue='Depression', ax=ax, palette='viridis', legend=False)\n    ax.set_title('Stress Finanziario vs Depressione')\n    ax.set_xlabel('Depressione (0=No, 1=S\u00ec)')\n    ax.set_ylabel('Stress Finanziario')\n    plt.xticks([0, 1], ['No', 'S\u00ec'])\n    plt.close(fig)\n    return fig\n\n\ndef plot_age_distribution_by_depression(df):\n    fig, ax = plt.subplots(figsize=(8, 5))\n    sns.histplot(data=df, x='Age', hue='Depression', kde=True, ax=ax, palette='coolwarm')\n    ax.set_title('Distribuzione Et\u00e0 per Condizione di Depressione')\n    ax.set_xlabel('Et\u00e0')\n    ax.set_ylabel('Frequenza')\n\n    handles, labels = ax.get_legend_handles_labels()\n    if handles: \n        if len(handles) == 2: # Assumendo 0='No', 1='S\u00ec'\n            ax.legend(handles, ['No', 'S\u00ec'], title='Depressione')\n        else:\n            ax.legend(handles=handles, labels=labels, title='Depressione')\n    plt.close(fig)\n    return fig\n\n\ndef plot_pearson_correlation(df):\n    fig, ax = plt.subplots(figsize=(10, 8))\n    numeric_df = df.select_dtypes(include=np.number)\n    if numeric_df.empty or numeric_df.shape[1] < 2 :\n        ax.text(0.5, 0.5, \"Dati numerici insufficienti per la correlazione.\",\n                ha='center', va='center', fontsize=12)\n        ax.axis('off'); plt.close(fig); return fig\n        \n    corr_matrix = numeric_df.corr()\n    sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', ax=ax, linewidths=.5)\n    ax.set_title('Matrice di Correlazione Pearson (Numeriche)')\n    plt.tight_layout(); plt.close(fig); return fig\n\n\ndef plot_depression_by_study_satisfaction(df):\n    fig, ax = plt.subplots(figsize=(7, 5))\n    sns.boxplot(data=df, x='Depression', y='Study Satisfaction', hue='Depression', ax=ax, palette='viridis', legend=False)\n    ax.set_title('Soddisfazione Studio vs Depressione')\n    ax.set_xlabel('Depressione (0=No, 1=S\u00ec)')\n    ax.set_ylabel('Soddisfazione Studio')\n    plt.xticks([0, 1], ['No', 'S\u00ec'])\n    plt.close(fig)\n    return fig\n\n\ndef plot_depression_by_status(df):\n    fig, ax = plt.subplots(figsize=(7, 5))\n    sns.countplot(data=df, x='Working Professional or Student', hue='Depression', ax=ax, palette='coolwarm')\n    ax.set_title('Depressione per Status (Studente/Professionista)')\n    ax.set_xlabel('Status')\n    ax.set_ylabel('Conteggio')\n    plt.xticks(rotation=45, ha='right') \n    \n    handles, labels = ax.get_legend_handles_labels()\n    if handles:\n        if len(handles) == 2:\n            ax.legend(handles, ['No', 'S\u00ec'], title='Depressione')\n        else:\n            ax.legend(handles=handles, labels=labels, title='Depressione')\n    plt.tight_layout(); plt.close(fig); return fig"
    },
    {
        "path": "C:/Users/Simxyz/Desktop/DataScienceCarreer/4.ItConsultingGiGroup/CorsoPythonwGithub/SimoneVerrengia_DepositoCorsoPython/MentallyStabilityOfThePerson-Prediction\\Mentally\\insertUtente.py",
        "name": "insertUtente.py",
        "content": "import pandas as pd\nfrom preprocessing import preprocess_person_test\nimport joblib\n\ndef gather_input(feature_columns):\n    \n    # Chiede all'utente di inserire i valori per ciascuna colonna non preprocessata.\n    # Validazione per colonne numeriche e visualizzazione opzioni per colonne categoriche.\n    # Restituisce un DataFrame con una singola riga comprensivo di 'id' dummy.\n    print(\"Inserisci i seguenti dati:\")\n    data = {'id': [0]}\n\n    # Range per colonne numeriche\n    numeric_ranges = {\n        'Age': (18, 60),\n        'Academic Pressure': (0, 5),\n        'Work Pressure': (0, 5),\n        'Study Satisfaction': (0, 5),\n        'Job Satisfaction': (0, 5),\n        'Financial Stress': (0, 5),\n        'CGPA': (0.0, 10.0),\n        'Sleep Duration': (0.0, 12.0),\n        'Work/Study Hours': (0.0, 12.0)\n    }\n\n    # Opzioni per colonne categoriche\n    categorical_options = {\n        'Gender': ['Male', 'Female'],\n        'Dietary Habits': ['Unhealthy', 'Moderate', 'Healthy'],\n        'Have you ever had suicidal thoughts ?': ['Yes', 'No'],\n        'Family History of Mental Illness': ['Yes', 'No']\n    }\n\n    for col in feature_columns:\n        # Costruisci prompt\n        if col in numeric_ranges:\n            lo, hi = numeric_ranges[col]\n            prompt = f\"- {col} (valore numerico tra {lo} e {hi}): \"\n        elif col in categorical_options:\n            opts = categorical_options[col]\n            prompt = f\"- {col} (scegli tra {opts}): \"\n        else:\n            prompt = f\"- {col}: \"\n\n        # Loop di validazione\n        while True:\n            raw = input(prompt).strip()\n            # Se colonna numerica\n            if col in numeric_ranges:\n                lo, hi = numeric_ranges[col]\n                try:\n                    val = float(raw) if isinstance(lo, float) else int(raw)\n                    if not (lo <= val <= hi):\n                        raise ValueError\n                except ValueError:\n                    print(f\"Input non valido. Inserisci un numero tra {lo} e {hi}.\")\n                    continue\n            # Se colonna categorica\n            elif col in categorical_options:\n                if raw not in categorical_options[col]:\n                    print(f\"Input non valido. Scegli una delle opzioni: {categorical_options[col]}\")\n                    continue\n                val = raw\n            # Altri tipi (es. stringhe libere)\n            else:\n                val = raw\n            break\n\n        data[col] = [val]\n\n    return pd.DataFrame(data)\n\ndef insert_data():\n    # Definisci qui le colonne raw (escludendo 'id')\n    feature_columns = [\n        'Name',\n        'Gender',\n        'Age',\n        'City',\n        'Working Professional or Student',\n        'Profession',\n        'Academic Pressure',\n        'Work Pressure',\n        'CGPA',\n        'Study Satisfaction',\n        'Job Satisfaction',\n        'Sleep Duration',\n        'Dietary Habits',\n        'Degree',\n        'Have you ever had suicidal thoughts ?',\n        'Work/Study Hours',\n        'Financial Stress',\n        'Family History of Mental Illness'\n    ]\n\n    # Raccogli i dati dell'utente\n    user_df = gather_input(feature_columns)\n\n    # Applica il preprocessing (ora user_df contiene 'id')\n    df_clean = preprocess_person_test(user_df)\n\n    # Carica il modello\n    model = joblib.load('best_xgb_clf_smote.pkl')\n\n# Predizione\n    y_pred = model.predict(df_clean)\n    proba = model.predict_proba(df_clean)[:, 1]  # Probabilit\u00e0 per la classe positiva\n\n    # Stampa il risultato per l'utente\n    if y_pred[0] == 1:\n        print(f\"Risultato: La persona potrebbe essere depressa (probabilit\u00e0: {proba[0]:.2%}).\")\n    else:\n        print(f\"Risultato: La persona non sembra depressa (probabilit\u00e0: {proba[0]:.2%}).\")\n"
    },
    {
        "path": "C:/Users/Simxyz/Desktop/DataScienceCarreer/4.ItConsultingGiGroup/CorsoPythonwGithub/SimoneVerrengia_DepositoCorsoPython/MentallyStabilityOfThePerson-Prediction\\Mentally\\mainTest.py",
        "name": "mainTest.py",
        "content": "import pandas as pd\nfrom preprocessing import preprocess_test\nimport joblib\n\ndef test():\n    test = pd.read_csv(r'Mentally\\test.csv')\n\n\n    print(\"Dati caricati con successo!\")\n    df_clean, test_ids = preprocess_test(test)\n    # --- Preparazione X_test e predizione ---\n    X_test = df_clean  # gi\u00e0 senza 'Depression'\n\n    model = joblib.load('best_xgb_clf_smote.pkl')\n    y_pred_class = model.predict(X_test)\n\n    # --- Creazione della submission ---\n    submission = pd.DataFrame({\n        'id': test_ids,\n        'Depression': y_pred_class\n    })\n\n    # --- Salvataggio su file ---\n    submission.to_csv('Mentally/submission.csv', index=False)\n    print(\"Submission salvata su 'Mentally/submission.csv'\")\n\n"
    },
    {
        "path": "C:/Users/Simxyz/Desktop/DataScienceCarreer/4.ItConsultingGiGroup/CorsoPythonwGithub/SimoneVerrengia_DepositoCorsoPython/MentallyStabilityOfThePerson-Prediction\\Mentally\\mainTrain.py",
        "name": "mainTrain.py",
        "content": "# Importiamo le librerie necessarie\nimport joblib\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom xgboost import XGBClassifier\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\nfrom imblearn.pipeline import Pipeline as ImbPipeline\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom preprocessing import preprocess_train, elimina_variabili_vif_pvalue\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n# Funzione per visualizzare pi\u00f9 matrici di confusione in un unico grafico\ndef plot_combined_confusion_matrices(y_true, y_pred_list, model_names):\n    plt.figure(figsize=(15, 4))\n    \n    for i, (y_pred, name) in enumerate(zip(y_pred_list, model_names)):\n        plt.subplot(1, 3, i+1)\n        cm = confusion_matrix(y_true, y_pred)\n        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n        plt.title(f\"Confusion Matrix - {name}\")\n        plt.ylabel('Actual')\n        plt.xlabel('Predicted')\n    \n    plt.tight_layout()\n    plt.show()\n    \n# Per un confronto ancora pi\u00f9 approfondito, possiamo anche aggiungere una visualizzazione \n# delle metriche principali in un unico grafico a barre\ndef plot_metrics_comparison(y_true, y_pred_list, model_names):\n    metrics = {\n        'Accuracy': [],\n        'Precision': [],\n        'Recall': [],\n        'F1 Score': []\n    }\n    \n    for y_pred in y_pred_list:\n        metrics['Accuracy'].append(accuracy_score(y_true, y_pred))\n        metrics['Precision'].append(precision_score(y_true, y_pred))\n        metrics['Recall'].append(recall_score(y_true, y_pred))\n        metrics['F1 Score'].append(f1_score(y_true, y_pred))\n    \n    # Creiamo un DataFrame per facilitare la visualizzazione\n    metrics_df = pd.DataFrame(metrics, index=model_names)\n    \n    # Plot\n    metrics_df.plot(kind='bar', figsize=(12, 6))\n    plt.title('Confronto delle metriche tra i modelli')\n    plt.ylabel('Score')\n    plt.ylim(0, 1.0)\n    plt.xticks(rotation=0)\n    plt.legend(loc='lower right')\n    plt.grid(axis='y', linestyle='--', alpha=0.7)\n    plt.tight_layout()\n    plt.show()\n    \n    return metrics_df\ndef train():\n    # Caricamento dei dati nella variabile train\n    train = pd.read_csv('Mentally/train.csv')\n    df_clean = preprocess_train(train)\n\n    # Selezione delle Feature e del Target\n    X = df_clean.drop(columns=['Depression'])\n    y = df_clean['Depression']\n    X_selected = elimina_variabili_vif_pvalue(X, y)\n    # Analizziamo la distribuzione delle classi\n    print(\"\\nDistribuzione delle classi nel target:\")\n    print(y.value_counts(normalize=True))\n\n    # Calcoliamo il rapporto per scale_pos_weight in XGBoost\n    negative_count, positive_count = np.bincount(y)\n    scale_pos_weight = negative_count / positive_count\n    print(f\"\\nRapporto delle classi (negativo/positivo): {scale_pos_weight:.2f}\")\n\n    # Separazione train/test mantenendo la distribuzione delle classi\n    X_train, X_test, y_train, y_test = train_test_split(\n        X_selected, y, \n        test_size=0.2, \n        random_state=73,\n        stratify=y  # Mantiene la distribuzione originale\n    )\n\n    # 1. Logistic Regression con class weights\n    log_reg = LogisticRegression(\n        random_state=73, \n        max_iter=1000,\n        class_weight='balanced'  # Bilanciamento classi\n    )\n\n    # 2. XGBoost con pesi per la classe positiva\n    xgb_clf = XGBClassifier(\n        objective='binary:logistic', \n        random_state=73,\n        scale_pos_weight=scale_pos_weight  # Bilanciamento classi\n    )\n\n    # 3. Pipeline con SMOTE e XGBoost\n    smote = SMOTE(random_state=73, sampling_strategy='auto')\n    xgb_smote = ImbPipeline([\n        ('smote', smote),\n        ('xgb', XGBClassifier(objective='binary:logistic', random_state=73))\n    ])\n\n    # Parametri per la GridSearch con SMOTE\n    param_grid = {\n        'xgb__n_estimators': [50, 100],\n        'xgb__max_depth': [3, 5],\n        'xgb__learning_rate': [0.01, 0.1],\n        'xgb__scale_pos_weight': [1, scale_pos_weight]  # Testa con e senza pesi\n    }\n\n    grid_clf = GridSearchCV(\n        xgb_smote,\n        param_grid, \n        scoring='f1', \n        cv=5, \n        n_jobs=-1\n    )\n\n    # Addestramento modelli\n    print(\"\\nAddestramento modelli...\")\n    log_reg.fit(X_train, y_train)\n    xgb_clf.fit(X_train, y_train)\n    grid_clf.fit(X_train, y_train)\n\n    # Miglior modello con SMOTE\n    best_xgb_clf = grid_clf.best_estimator_\n\n    # Predizioni\n    y_pred_log = log_reg.predict(X_test)\n    y_pred_xgb = xgb_clf.predict(X_test)\n    y_pred_xgb_best = best_xgb_clf.predict(X_test)\n\n    # Salvataggio modelli\n    joblib.dump(best_xgb_clf, 'best_xgb_clf_smote.pkl')\n    print(\"Modello XGB ottimizzato (SMOTE) salvato in 'best_xgb_clf_smote.pkl'\")\n\n    joblib.dump(log_reg, 'logistic_regression_smote.pkl')\n    joblib.dump(xgb_clf, 'xgb_clf_default_smote.pkl')\n    print(\"Modelli log_reg e xgb_clf default salvati in 'logistic_regression_smote.pkl' e 'xgb_clf_default_smote.pkl'\")\n\n    # Visualizzazione e confronto\n    plot_combined_confusion_matrices(\n        y_test, \n        [y_pred_log, y_pred_xgb, y_pred_xgb_best], \n        [\"Logistic Regression\", \"XGBoost\", \"XGBoost (Optimized)\"]\n    )\n\n    metrics_df = plot_metrics_comparison(\n        y_test, \n        [y_pred_log, y_pred_xgb, y_pred_xgb_best], \n        [\"Logistic Regression\", \"XGBoost\", \"XGBoost (Optimized)\"]\n    )\n\n    # Distribuzione delle classi\n    print(\"\\nDistribuzione delle classi nel target dopo lo SMOTE:\")\n    print(y.value_counts(normalize=True))\n\n    print(\"\\nConfronti metriche in formato tabella:\")\n    print(metrics_df.round(4))\n"
    },
    {
        "path": "C:/Users/Simxyz/Desktop/DataScienceCarreer/4.ItConsultingGiGroup/CorsoPythonwGithub/SimoneVerrengia_DepositoCorsoPython/MentallyStabilityOfThePerson-Prediction\\Mentally\\mentallyApp.py",
        "name": "mentallyApp.py",
        "content": "import os\nimport sys\nimport threading\nimport pandas as pd\nimport joblib\nimport numpy as np\nimport graphicGui\nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.pipeline import Pipeline as ImbPipeline\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom preprocessing import preprocess_train, elimina_variabili_vif_pvalue, preprocess_person_test\n\nfrom PyQt5.QtCore import Qt, pyqtSignal, QObject\nfrom PyQt5.QtWidgets import (\n    QApplication, QMainWindow, QWidget, QStackedWidget,\n    QVBoxLayout, QHBoxLayout, QFormLayout,\n    QLabel, QPushButton, QFileDialog, QMessageBox,\n    QLineEdit, QTextEdit, QComboBox\n)\nfrom matplotlib.backends.backend_qt5agg import FigureCanvasQTAgg as FigureCanvas\n\n# Costanti\nCLEANED_CSV = 'Mentally/data/cleaned_data_for_graphs.csv'\nDEFAULT_MODEL_FILE = 'Mentally/modelli/best_xgb_clf_smote.pkl'\n\n\nclass Worker(QObject):\n    status = pyqtSignal(str)\n    finished = pyqtSignal()\n\n    def __init__(self, data_path):\n        super().__init__()\n        self.data_path = data_path\n\n    def run(self):\n        try:\n            self.status.emit(\"Caricamento dati...\")\n            df = pd.read_csv(self.data_path)\n            \n            #os.makedirs('Mentally/data', exist_ok=True)\n            \n            self.status.emit(\"Preprocessing...\")\n            df_clean = preprocess_train(df)\n            df_clean.to_csv(CLEANED_CSV, index=False)\n\n            X = df_clean.drop(columns=['Depression'])\n            y = df_clean['Depression']\n            self.status.emit(\"Selezione variabili...\")\n            X_sel = elimina_variabili_vif_pvalue(X.copy(), y.copy()) \n\n            self.status.emit(\"Preparazione modelli...\")\n            counts = np.bincount(y)\n            if len(counts) < 2:\n                 self.status.emit(\"Errore: La colonna target 'Depression' contiene solo una classe dopo il preprocessing.\")\n                 self.finished.emit()\n                 return\n            neg, pos = counts\n            scale = neg / pos if pos > 0 else 1.0 # scale_pos_weight\n\n            X_train, X_test, y_train, y_test = train_test_split(\n                X_sel, y, test_size=0.2, random_state=73, stratify=y\n            )\n\n            log_reg = LogisticRegression(random_state=73, max_iter=1000, class_weight='balanced')\n            xgb_model_default = XGBClassifier(objective='binary:logistic', random_state=73, scale_pos_weight=scale, eval_metric='logloss')\n            \n            smote = SMOTE(random_state=73)\n            pipeline_xgb_smote = ImbPipeline([\n                ('smote', smote),\n                ('xgb', XGBClassifier(objective='binary:logistic', random_state=73, eval_metric='logloss'))\n            ])\n            \n            param_grid_xgb_smote = {\n                'xgb__n_estimators': [50, 100], # Valori ridotti per test pi\u00f9 rapidi, espandere se necessario\n                'xgb__max_depth': [3, 5],\n                'xgb__learning_rate': [0.05, 0.1],\n                'xgb__scale_pos_weight': [1, scale] \n            }\n            grid_search_cv = GridSearchCV(pipeline_xgb_smote, param_grid_xgb_smote, scoring='f1', cv=3, n_jobs=-1) # cv ridotto per test pi\u00f9 rapidi\n\n            self.status.emit(\"Addestramento modelli...\")\n            log_reg.fit(X_train, y_train)\n            self.status.emit(\"Logistic Regression addestrato.\")\n            xgb_model_default.fit(X_train, y_train)\n            self.status.emit(\"XGBoost di default addestrato.\")\n            grid_search_cv.fit(X_train, y_train)\n            self.status.emit(\"GridSearchCV per XGBoost con SMOTE completato.\")\n\n            best_xgb_smote_model = grid_search_cv.best_estimator_\n            \n            #os.makedirs('Mentally/modelli', exist_ok=True)\n            joblib.dump(best_xgb_smote_model, DEFAULT_MODEL_FILE)\n            joblib.dump(log_reg, 'Mentally/modelli/logistic_regression_balanced.pkl')\n            joblib.dump(xgb_model_default, 'Mentally/modelli/xgb_clf_default_scaled.pkl')\n\n            self.status.emit(\"Addestramento completato.\")\n            self.status.emit(\"Salvataggio modelli completato.\")\n            \n            self.status.emit(\"\\n--- Valutazione Modelli su Test Set ---\")\n\n            models_to_evaluate = {\n                \"Logistic Regression (balanced)\": log_reg,\n                \"XGBoost (default with scale_pos_weight)\": xgb_model_default,\n                \"Best XGBoost (GridSearchCV with SMOTE)\": best_xgb_smote_model\n            }\n\n            y_true = y_test\n\n            for model_name, model_instance in models_to_evaluate.items():\n                y_pred = model_instance.predict(X_test)\n\n                accuracy = accuracy_score(y_true, y_pred)\n                precision = precision_score(y_true, y_pred, zero_division=0)\n                recall = recall_score(y_true, y_pred, zero_division=0)\n                f1 = f1_score(y_true, y_pred, zero_division=0)\n\n                self.status.emit(f\"\\nMetriche per il modello: {model_name}\")\n\n                # elenco delle metriche\n                for label, val in [\n                    (\"Accuracy\",  accuracy),\n                    (\"Precision\", precision),\n                    (\"Recall\",    recall),\n                    (\"F1 Score\",  f1)\n                ]:\n                    self.status.emit(f\"{label}:\\t{val:.4f}\")\n\n\n            self.status.emit(\"\\nValutazione metriche completata.\")\n\n        except Exception as e:\n            self.status.emit(f\"Errore: {e}\")\n            import traceback\n            self.status.emit(traceback.format_exc())\n        finally:\n            self.finished.emit()\n\nclass TrainPage(QWidget):\n    def __init__(self, parent=None):\n        super().__init__(parent)\n        layout = QVBoxLayout(self)\n\n        form = QHBoxLayout()\n        self.path_edit = QLineEdit()\n        btn_browse = QPushButton(\"Sfoglia...\")\n        form.addWidget(QLabel(\"Percorso CSV:\"))\n        form.addWidget(self.path_edit)\n        form.addWidget(btn_browse)\n        layout.addLayout(form)\n\n        self.start_btn = QPushButton(\"Avvia Addestramento\")\n        layout.addWidget(self.start_btn)\n\n        self.log = QTextEdit()\n        self.log.setReadOnly(True)\n        layout.addWidget(self.log)\n\n        # self.back_btn = QPushButton(\"Indietro\")\n        # layout.addWidget(self.back_btn)\n\n        btn_browse.clicked.connect(self.browse)\n        self.start_btn.clicked.connect(self.start)\n\n    def browse(self):\n        fp, _ = QFileDialog.getOpenFileName(self, \"Seleziona CSV\", \".\", \"CSV files (*.csv)\")\n        if fp:\n            self.path_edit.setText(fp)\n\n    def start(self):\n        path = self.path_edit.text().strip()\n        if not os.path.exists(path):\n            QMessageBox.warning(self, \"Errore\", \"File non esistente\")\n            return\n\n        self.log.clear()\n        self.start_btn.setEnabled(False)\n\n        self.worker = Worker(path)\n        thread = threading.Thread(target=self.worker.run)\n        self.worker.status.connect(self.log.append)\n        self.worker.finished.connect(lambda: self.start_btn.setEnabled(True))\n        thread.start()\n\n\nclass GraphsPage(QWidget):\n    def __init__(self, parent=None):\n        super().__init__(parent)\n        layout = QVBoxLayout(self)\n\n        # Top: select CSV\n        top = QHBoxLayout()\n        self.lbl_datafile = QLabel(f\"Dati: {CLEANED_CSV}\")\n        btn_load_csv = QPushButton(\"Carica Dati\u2026\")\n        top.addWidget(self.lbl_datafile)\n        top.addStretch()\n        top.addWidget(btn_load_csv)\n        layout.addLayout(top)\n\n        # Body: buttons + canvas\n        body = QHBoxLayout()\n        self.btn_layout = QVBoxLayout()\n        self.canvas_holder = QVBoxLayout()\n        body.addLayout(self.btn_layout)\n        body.addLayout(self.canvas_holder)\n        layout.addLayout(body)\n\n        # self.back_btn = QPushButton(\"Indietro\")\n        # layout.addWidget(self.back_btn, alignment=Qt.AlignRight)\n\n        self.data_file = CLEANED_CSV\n        btn_load_csv.clicked.connect(self.browse_csv)\n\n        self.graph_funcs = {\n            \"Distribuzione Genere\":      graphicGui.plot_gender_distribution,\n            \"Studente vs Professionista\":graphicGui.plot_status_distribution,\n            \"Condizione di Depressione\": graphicGui.plot_depression_distribution,\n            \"Depressione per Genere\":    graphicGui.plot_depression_by_gender,\n            \"Stress Finanziario\":        graphicGui.plot_financial_stress_by_depression,\n            \"Et\u00e0 vs Depress.\":           graphicGui.plot_age_distribution_by_depression,\n            \"Correlaz. Pearson\":         graphicGui.plot_pearson_correlation,\n            \"Depress. per Laurea\":       graphicGui.plot_depression_by_Degree_Group_Encoded_group,\n            \"Soddisf. Studio\":           graphicGui.plot_depression_by_study_satisfaction,\n            \"Depress. per Status\":       graphicGui.plot_depression_by_status\n        }\n\n        for name in self.graph_funcs:\n            btn = QPushButton(name)\n            self.btn_layout.addWidget(btn)\n           \n            \n            btn.clicked.connect(lambda _, name=name: self.show_graph(name))\n           \n\n    def browse_csv(self):\n        fp, _ = QFileDialog.getOpenFileName(self, \"Seleziona CSV pulito\", \".\", \"CSV files (*.csv)\")\n        if fp:\n            self.data_file = fp\n            self.lbl_datafile.setText(f\"Dati: {os.path.basename(fp)}\")\n\n    def load_data(self):\n        if not os.path.exists(self.data_file):\n            return None\n        try:\n            return pd.read_csv(self.data_file)\n        except:\n            return None\n\n    def show_graph(self, name):\n        \n        df = self.load_data()\n        if df is None:\n            QMessageBox.warning(self, \"Errore\", \"Dati non disponibili. Carica prima un CSV.\")\n            return\n\n        fig = self.graph_funcs[name](df)\n\n        # remove old widget\n        for i in reversed(range(self.canvas_holder.count())):\n            w = self.canvas_holder.itemAt(i).widget()\n            if w:\n                w.setParent(None)\n\n        canvas = FigureCanvas(fig)\n        self.canvas_holder.addWidget(canvas)\n\n\nclass PredictPage(QWidget):\n    def __init__(self, parent=None):\n        super().__init__(parent)\n        layout = QVBoxLayout(self)\n\n        # Model loader\n        hmod = QHBoxLayout()\n        self.lbl_model = QLabel(f\"Modello: {os.path.basename(DEFAULT_MODEL_FILE)}\")\n        btn_load = QPushButton(\"Carica Modello\u2026\")\n        hmod.addWidget(self.lbl_model)\n        hmod.addStretch()\n        hmod.addWidget(btn_load)\n        layout.addLayout(hmod)\n\n        self.model = None\n        self.load_model(DEFAULT_MODEL_FILE)\n        btn_load.clicked.connect(self.browse_model)\n\n        # Features form\n        self.features = {\n            'Name': {'type':'text'},\n            'Gender': {'type':'categorical','options':['Male','Female']},\n            'Age': {'type':'numeric','range':(18,60)},\n            'City': {'type':'text'},\n            'Working Professional or Student':{'type':'text'},\n            'Profession':{'type':'text'},\n            'Academic Pressure':{'type':'numeric','range':(0,5)},\n            'Work Pressure':{'type':'numeric','range':(0,5)},\n            'CGPA':{'type':'numeric','range':(0.0,10.0)},\n            'Study Satisfaction':{'type':'numeric','range':(0,5)},\n            'Job Satisfaction':{'type':'numeric','range':(0,5)},\n            'Sleep Duration':{'type':'numeric','range':(0.0,12.0)},\n            'Dietary Habits':{'type':'categorical','options':['Unhealthy','Moderate','Healthy']},\n            'Degree':{'type':'text'},\n            'Have you ever had suicidal thoughts ?':{'type':'categorical','options':['Yes','No']},\n            'Work/Study Hours':{'type':'numeric','range':(0.0,12.0)},\n            'Financial Stress':{'type':'numeric','range':(0,5)},\n            'Family History of Mental Illness':{'type':'categorical','options':['Yes','No']}\n        }\n        form = QFormLayout()\n        self.inputs = {}\n        for col, defn in self.features.items():\n            if defn['type']=='categorical':\n                cb = QComboBox()\n                cb.addItems(defn['options'])\n                form.addRow(col, cb)\n                self.inputs[col] = cb\n            else:\n                le = QLineEdit()\n                label = f\"{col} ({defn['range'][0]}-{defn['range'][1]})\" if defn['type']=='numeric' else col\n                form.addRow(label, le)\n                self.inputs[col] = le\n        layout.addLayout(form)\n\n        self.btn_pred = QPushButton(\"Esegui Predizione\")\n        self.lbl_res  = QLabel(\"Inserisci i dati e premi Predizione\")\n        # btn_back      = QPushButton(\"Indietro\")\n        layout.addWidget(self.btn_pred)\n        layout.addWidget(self.lbl_res)\n        # layout.addWidget(btn_back)\n\n        self.btn_pred.clicked.connect(self.perform_prediction)\n        # btn_back.clicked.connect(lambda: self.parent().setCurrentIndex(0))\n\n    def browse_model(self):\n        fp, _ = QFileDialog.getOpenFileName(self, \"Seleziona modello .pkl\", \".\", \"Pickle files (*.pkl)\")\n        if fp:\n            try:\n                self.model = joblib.load(fp)\n                self.lbl_model.setText(f\"Modello: {os.path.basename(fp)}\")\n                QMessageBox.information(self, \"Modello Caricato\", f\"Caricato: {os.path.basename(fp)}\")\n            except Exception as e:\n                QMessageBox.critical(self, \"Errore Caricamento\", str(e))\n\n    def load_model(self, path):\n        try:\n            self.model = joblib.load(path)\n        except Exception as e:\n            QMessageBox.critical(self, \"Errore\", f\"Impossibile caricare default: {e}\")\n\n    def perform_prediction(self):\n        data = {'id':[0]}\n        errors = []\n        for col, defn in self.features.items():\n            widget = self.inputs[col]\n            raw = widget.currentText() if defn['type']=='categorical' else widget.text().strip()\n            if defn['type']=='numeric':\n                if not raw:\n                    errors.append(f\"'{col}' mancante\")\n                    continue\n                try:\n                    val = float(raw) if isinstance(defn['range'][0], float) else int(raw)\n                except:\n                    errors.append(f\"'{col}' non valido\")\n                    continue\n                data[col] = [val]\n            else:\n                data[col] = [raw]\n        if errors:\n            QMessageBox.warning(self, \"Errori di input\", \"\\n\".join(errors))\n            return\n        df = pd.DataFrame(data)\n        try:\n            df_clean = preprocess_person_test(df)\n            y = self.model.predict(df_clean)\n            if hasattr(self.model, 'predict_proba'):\n                p = self.model.predict_proba(df_clean)[:,1][0]\n                text = f\"Depressione: {'SI' if y[0]==1 else 'NO'} (Prob: {p:.2%})\"\n            else:\n                text = f\"Depressione: {'SI' if y[0]==1 else 'NO'}\"\n            self.lbl_res.setText(text)\n        except Exception as e:\n            QMessageBox.critical(self, \"Errore Predizione\", str(e))\n\n\nclass DepressionAnalysisApp(QMainWindow):\n    def __init__(self):\n        super().__init__()\n        self.setWindowTitle(\"Analisi e Predizione Depressione\")\n        self.resize(900, 700)\n\n        main_widget = QWidget()\n        main_layout = QVBoxLayout(main_widget)\n        self.setCentralWidget(main_widget)\n\n        # Barra di navigazione in alto\n        nav_bar = QWidget()\n        nav_layout = QHBoxLayout(nav_bar)\n        nav_layout.setContentsMargins(5, 5, 5, 5)\n        self.btn_train  = QPushButton(\"Addestra Modelli\")\n        self.btn_graphs = QPushButton(\"Visualizza Grafici\")\n        self.btn_predict= QPushButton(\"Predizione Singola\")\n        nav_layout.addWidget(self.btn_train)\n        nav_layout.addWidget(self.btn_graphs)\n        nav_layout.addWidget(self.btn_predict)\n        nav_layout.addStretch()\n        main_layout.addWidget(nav_bar)\n\n        # Stack delle pagine\n        self.stack = QStackedWidget()\n        self.train_page   = TrainPage(self)\n        self.graphs_page  = GraphsPage(self)\n        self.predict_page = PredictPage(self)\n        self.stack.addWidget(self.train_page)\n        self.stack.addWidget(self.graphs_page)\n        self.stack.addWidget(self.predict_page)\n        main_layout.addWidget(self.stack)\n\n        # Connect navigation\n        self.btn_train.clicked.connect(lambda: self.stack.setCurrentWidget(self.train_page))\n        self.btn_graphs.clicked.connect(lambda: self.stack.setCurrentWidget(self.graphs_page))\n        self.btn_predict.clicked.connect(lambda: self.stack.setCurrentWidget(self.predict_page))\n\n\nif __name__ == '__main__':\n    app = QApplication(sys.argv)\n    window = DepressionAnalysisApp()\n    window.show()\n    sys.exit(app.exec_())\n"
    },
    {
        "path": "C:/Users/Simxyz/Desktop/DataScienceCarreer/4.ItConsultingGiGroup/CorsoPythonwGithub/SimoneVerrengia_DepositoCorsoPython/MentallyStabilityOfThePerson-Prediction\\Mentally\\menu.py",
        "name": "menu.py",
        "content": "from mainTest import test\nfrom mainTrain import train\nfrom insertUtente import insert_data\nfrom grafici import menu_visualizzazioni\n\ndef menu():\n    while True:\n        print(\"Benvenuto nel menu principale!\")\n        print(\"1. Analisi, preprocessing, addestramento e previsione su file CSV di training\")\n        print(\"2. Analisi, preprocessing, previsione su test e generazione submission\" )\n        print(\"3. Predizione dello stato depressivo di una persona (inserimento manuale)\")\n        print(\"4. Visualizza grafici\")\n        print(\"5. Esci\")\n\n    \n        choice = input(\"Scegli un'opzione (1-4): \").strip()\n        if choice == '1':\n            train()\n        elif choice == '2':\n            test()\n        elif choice == '3':\n            insert_data()\n        elif choice == '4':\n            menu_visualizzazioni()\n        elif choice == '5':\n            print(\"Uscita dal programma.\")\n            break\n        else:\n            print(\"Opzione non valida. Riprova.\")\n\nif __name__ == \"__main__\":   \n    menu()\n"
    },
    {
        "path": "C:/Users/Simxyz/Desktop/DataScienceCarreer/4.ItConsultingGiGroup/CorsoPythonwGithub/SimoneVerrengia_DepositoCorsoPython/MentallyStabilityOfThePerson-Prediction\\Mentally\\preprocessing.py",
        "name": "preprocessing.py",
        "content": "import re\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import OrdinalEncoder, LabelEncoder\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# --- Codifica 'Sleep Duration' ---\n# Funzione per mappare le stringhe di durata del sonno a valori numerici\ndef map_sleep_duration(duration_str):\n    s = str(duration_str).strip()  # Converti in stringa e rimuovi spazi\n\n    # Casi speciali (es. \"Less than 5\", \"Under 4\")\n    if re.search(r'less.*?\\d+|under.*?\\d+', s, re.IGNORECASE):\n        num = re.search(r'\\d+', s)\n        return float(num.group()) - 1 if num else np.nan\n\n    # Casi speciali (es. \"More than 8\", \"Over 9\")\n    elif re.search(r'more.*?\\d+|over.*?\\d+', s, re.IGNORECASE):\n        num = re.search(r'\\d+', s)\n        return float(num.group()) + 1 if num else np.nan\n\n    # Intervallo (es. \"6-7\", \"4\u20135 hours\")\n    range_match = re.search(r'(\\d+)\\s*[-\u2013\u2014]\\s*(\\d+)', s)\n    if range_match:\n        num1, num2 = map(float, range_match.groups())\n        avg = (num1 + num2) / 2\n        return avg if avg <= 24 else float(str(int(avg))[0])  # correzione se media troppo alta\n\n    # Singolo numero (es. \"7\", \"about 6.5\")\n    num_match = re.search(r'\\d+\\.?\\d*', s)\n    if num_match:\n        val = float(num_match.group())\n        # Corregge se il valore \u00e8 palesemente errato (>24 ore)\n        if val > 24:\n            return float(str(int(val))[0])  # Es. \"49\" -> \"4\"\n        return val\n\n    return np.nan\n\n# Utilizziamo la funzione per calcolare il VIF ed il Pvalue sulle Feature per verificare eventuali rimozioni\ndef elimina_variabili_vif_pvalue(X, y, vif_threshold=5.0, pvalue_threshold=0.05):\n    \n    X_current = X.copy()\n    \n    while True:\n        X_const = sm.add_constant(X_current)\n        model = sm.OLS(y, X_const).fit()\n        pvals = model.pvalues.drop('const')\n        vif_data = pd.DataFrame({\n            'Feature': X_current.columns,\n            'VIF': [variance_inflation_factor(X_current.values, i) \n                    for i in range(X_current.shape[1])],\n            'p-value': pvals.values\n        })\n        cond = (vif_data['VIF'] > vif_threshold) & (vif_data['p-value'] > pvalue_threshold)\n        print(vif_data[['VIF','p-value']])\n        if not cond.any():\n            break\n        # Rimuovo la variabile con VIF pi\u00f9 alto\n        to_remove = vif_data.loc[cond, 'Feature'].iloc[vif_data.loc[cond,'VIF'].argmax()]\n        print(f\"Rimuovo {to_remove} (VIF={vif_data.loc[vif_data.Feature==to_remove,'VIF'].values[0]:.2f}, \"\n            f\"p-val={vif_data.loc[vif_data.Feature==to_remove,'p-value'].values[0]:.4f})\")\n        X_current.drop(columns=[to_remove], inplace=True)\n    print(\"Feature finali:\", X_current.columns.tolist())\n    print(\"Numero di feature:\", len(X_current.columns))\n    \n    return X_current\n\ndef preprocess_train(df):\n    # Preparazione del DataFrame per l'imputazione\n    # --- Imputazione della colonna 'Profession' ---\n    # Applichiamo la seguente logica:\n    # - Se 'Working Professional or Student' \u00e8 valorizzato 'Student' E 'Profession' \u00e8 NaN, imputiamo con 'Student'.\n    # - Se 'Working Professional or Student' \u00e8 valorizzato 'Working Professional' E 'Profession' \u00e8 NaN, imputa con 'Unknown'.\n\n    df.loc[\n        (df['Working Professional or Student'] == 'Student') & (df['Profession'].isna()),\n        'Profession'\n    ] = 'Student'\n\n    df.loc[\n        (df['Working Professional or Student'] == 'Working Professional') & (df['Profession'].isna()),\n        'Profession'\n    ] = 'Unknown'\n\n    # Imputiamo con 0 i mancanti nelle colonne accademiche SOLO per i 'Working Professional'.\n    academic_cols = ['Academic Pressure', 'CGPA', 'Study Satisfaction']\n    for col in academic_cols:\n        # Seleziona le righe dove lo stato \u00e8 'Working Professional' e la colonna corrente \u00e8 NaN\n        df.loc[\n            (df['Working Professional or Student'] == 'Working Professional') & (df[col].isna()),\n            col\n        ] = 0\n\n    # Imputiamo con 0 i mancanti nelle colonne lavorative SOLO per gli 'Student'.\n    work_cols = ['Work Pressure', 'Job Satisfaction']\n    for col in work_cols:\n        # Seleziona le righe dove lo stato \u00e8 'Student' e la colonna corrente \u00e8 NaN\n        df.loc[\n            (df['Working Professional or Student'] == 'Student') & (df[col].isna()),\n            col\n        ] = 0\n\n    # --- Gestione dei rimanenti mancanti nelle colonne applicabili ---\n    # Imputiamo i rimanenti mancanti nelle colonne accademiche per gli 'Studenti' con la mediana del sottogruppo 'Student'.\n    for col in academic_cols:\n        # Seleziona le righe dove lo stato \u00e8 'Student' e la colonna corrente \u00e8 NaN\n        condition = (df['Working Professional or Student'] == 'Student') & (df[col].isna())\n        mediana_sottogruppo = df[df['Working Professional or Student'] == 'Student'][col].median()\n        df.loc[condition, col] = mediana_sottogruppo\n\n    # Imputa i rimanenti mancanti nelle colonne lavorative per i 'Working Professional' con la mediana del sottogruppo 'Working Professional'.\n    for col in work_cols:\n        # Seleziona le righe dove lo stato \u00e8 'Working Professional' e la colonna corrente \u00e8 NaN\n        condition = (df['Working Professional or Student'] == 'Working Professional') & (df[col].isna())\n        # Rimossa la condizione if df.loc[condition].shape[0] > 0:\n        mediana_sottogruppo = df[df['Working Professional or Student'] == 'Working Professional'][col].median()\n        df.loc[condition, col] = mediana_sottogruppo\n\n    # --- Gestione dei mancanti nelle altre colonne con pochi valori assenti ---\n    # Imputazione con la moda per 'Dietary Habits' e 'Degree' (categoriche)\n    for col in ['Dietary Habits', 'Degree']:\n        if col in df.columns and not df[col].empty:\n            # Calcola la moda solo se ci sono valori non-null\n            if df[col].notna().any():\n                moda_val = df[col].mode()[0]\n                df[col] = df[col].fillna(moda_val)\n            else:\n                print(f\"Attenzione: La colonna '{col}' contiene solo valori NaN. Imputazione con moda non possibile.\")\n\n    # Imputazione con la mediana per 'Financial Stress' (numerica)\n    if 'Financial Stress' in df.columns and not df['Financial Stress'].empty:\n        # Calcola la mediana solo se ci sono valori non-null\n        if df['Financial Stress'].notna().any():\n            mediana_val = df['Financial Stress'].median()\n            df['Financial Stress'] = df['Financial Stress'].fillna(mediana_val)\n        else:\n            print(\"Attenzione: La colonna 'Financial Stress' contiene solo valori NaN. Imputazione con mediana non possibile.\")\n\n    # --- Codifica delle variabili categoriche ---\n    # Codifica 'Gender': Male=1, Female=0\n    df['Gender'] = df['Gender'].map({'Male': 1, 'Female': 0})\n    # Controlla se ci sono valori non mappati dopo la trasformazione (potrebbero diventare NaN)\n    if df['Gender'].isna().any():\n        print(\"Attenzione: Valori non previsti nella colonna 'Gender' dopo la mappatura.\")\n\n    # Codifica 'Working Professional or Student': Working Professional=1, Student=0\n    df['Working Professional or Student'] = df['Working Professional or Student'].map({'Working Professional': 1, 'Student': 0})\n    # Controlla se ci sono valori non mappati\n    if df['Working Professional or Student'].isna().any():\n        print(\"Attenzione: Valori non previsti nella colonna 'Working Professional or Student' dopo la mappatura.\")\n\n    df['Sleep Duration'] = df['Sleep Duration'].apply(map_sleep_duration)\n\n    # Controlla se ci sono valori non mappati (dovrebbero essere NaN se la funzione non li ha riconosciuti)\n    if df['Sleep Duration'].isna().any():\n        print(\"Attenzione: Valori non previsti nella colonna 'Sleep Duration' dopo la mappatura. Potrebbero essere NaN.\")\n        mediana_sleep = df['Sleep Duration'].median()\n        df['Sleep Duration'] = df['Sleep Duration'].fillna(mediana_sleep)\n\n    # --- Pulizia e Codifica 'Degree' ---\n    # Standardizziamo spazi e rimuoviamo punteggiatura non necessaria\n    df['Degree'] = (\n        df['Degree']\n        .astype(str)\n        .str.strip()\n        .str.replace(r'\\s+', ' ', regex=True) # Standardizza spazi multipli in singolo spazio\n        .str.replace(r'[^\\w\\s.-]', '', regex=True) # Rimuove caratteri che non sono parola, spazio, punto o trattino\n        .str.title() # Mette in maiuscolo la prima lettera di ogni parola\n    )\n\n    # Mappa in gruppi (aggiornato per gestire i valori puliti)\n    mapping_groups = {\n        # High School / Diploma\n        'Class 11': 'High School',\n        'Class 12': 'High School',\n        'Diploma': 'High School',\n        # Bachelor\n        'B.Tech': 'Bachelor', 'Btech': 'Bachelor',\n        'B.Sc': 'Bachelor', 'Bsc': 'Bachelor',\n        'B.Com': 'Bachelor', 'Bcom': 'Bachelor',\n        'Bca': 'Bachelor', 'Ba': 'Bachelor', 'Bba': 'Bachelor', 'Bed': 'Bachelor',\n        'B.Arch': 'Bachelor', 'Barch': 'Bachelor',\n        'B.Pharm': 'Bachelor', 'Bpharm': 'Bachelor',\n        'Bdes': 'Bachelor', 'Bfa': 'Bachelor', 'Bhm': 'Bachelor', 'Bpt': 'Bachelor',\n        'Bds': 'Bachelor', 'Bams': 'Bachelor', 'Bhms': 'Bachelor', 'Bums': 'Bachelor',\n        'B.A.': 'Bachelor', 'B.Com.': 'Bachelor', 'B.Sc.': 'Bachelor', # Aggiunti con punti\n        'B.B.A.': 'Bachelor', 'B.C.A.': 'Bachelor', 'B.Ed.': 'Bachelor',\n        # Master\n        'M.Tech': 'Master', 'Mtech': 'Master',\n        'M.Sc': 'Master', 'Msc': 'Master',\n        'M.Com': 'Master', 'Mcom': 'Master',\n        'Mca': 'Master', 'M.Ed': 'Master', 'Med': 'Master',\n        'M.Pharm': 'Master', 'Mpharm': 'Master',\n        'Mba': 'Master', 'Mdes': 'Master', 'Mfa': 'Master', 'Mhm': 'Master',\n        'Mpt': 'Master', 'Mds': 'Master', 'Mams': 'Master', 'Mhms': 'Master',\n        'M.A.': 'Master', 'M.Com.': 'Master', 'M.Sc.': 'Master', # Aggiunti con punti\n        'M.B.A.': 'Master', 'M.C.A.': 'Master', 'M.Ed.': 'Master',\n        # Doctorate / Professional\n        'Phd': 'Doctorate', 'Mbbs': 'Doctorate', 'Md': 'Doctorate', 'Llm': 'Doctorate',\n        'Ll.B.Ed': 'Doctorate', 'Ll.Ba': 'Doctorate',\n        'D.Phil': 'Doctorate', 'Dr': 'Doctorate', 'Ph.D.': 'Doctorate', # Aggiunti\n        'M.D.': 'Doctorate', 'L.L.M.': 'Doctorate',\n        # Aggiungi altri se necessario in base all'esplorazione dei dati\n    }\n\n    # Applica il mapping; i non mappati diventano 'Other'\n    df['Degree_Group'] = df['Degree'].map(mapping_groups).fillna('Other')\n\n    # --- Codifica 'Degree_Group' con OrdinalEncoder ---\n    print(\"\\nEseguendo Ordinal Encoding per la colonna 'Degree_Group'...\")\n\n    # Definisci l'ordine delle categorie per il livello di istruzione\n    # Includiamo 'Other' come livello pi\u00f9 basso per l'ordinamento.\n    degree_order = ['Other', 'High School', 'Bachelor', 'Master', 'Doctorate']\n\n    # Inizializza OrdinalEncoder con l'ordine specificato\n    ordinal_encoder_degree = OrdinalEncoder(categories=[degree_order])\n\n    # Fit e trasforma la colonna 'Degree_Group'\n    # Reshape necessario perch\u00e9 fit_transform si aspetta un input 2D\n    df['Degree_Group_Encoded'] = ordinal_encoder_degree.fit_transform(df[['Degree_Group']])\n\n    # --- Codifica 'Dietary Habits' con OrdinalEncoder ---\n    print(\"\\nValori unici nella colonna 'Dietary Habits' prima della codifica:\")\n    print(df['Dietary Habits'].unique())\n\n    # Definisci i valori validi per 'Dietary Habits'\n    valid_dietary_habits = ['Unhealthy', 'Moderate', 'Healthy']\n\n    # Elimina le righe dove 'Dietary Habits' non \u00e8 uno dei valori validi E non \u00e8 NaN\n    # Vogliamo le righe dove il valore NON \u00e8 'Unhealthy' AND NON \u00e8 'Moderate' AND NON \u00e8 'Healthy'\n    # E il valore NON \u00e8 NaN (per evitare di eliminare i NaN originali se non imputati)\n    condition_not_valid = (df['Dietary Habits'] != 'Unhealthy') & \\\n                        (df['Dietary Habits'] != 'Moderate') & \\\n                        (df['Dietary Habits'] != 'Healthy') & \\\n                        df['Dietary Habits'].notna()\n\n    rows_to_drop = df[condition_not_valid].index\n\n    print(f\"\\nNumero di righe da eliminare nella colonna 'Dietary Habits': {len(rows_to_drop)}\")\n\n    # Elimina le righe identificate\n    df = df.drop(rows_to_drop)\n\n    print(f\"Numero di righe rimanenti nel DataFrame: {len(df)}\")\n\n    # Ordine: Unhealthy < Moderate < Healthy\n    encoder = OrdinalEncoder(categories=[valid_dietary_habits])\n\n    try:\n        # Modifica per usare solo i valori validi definiti\n        encoder = OrdinalEncoder(categories=[valid_dietary_habits])\n        df['Dietary Habits'] = encoder.fit_transform(df[['Dietary Habits']])\n        print(\"\\nCodifica 'Dietary Habits' completata con successo.\")\n    except ValueError as e:\n        print(f\"\\nErrore durante la codifica di 'Dietary Habits': {e}\")\n        print(\"Controlla i valori unici stampati sopra. La colonna 'Dietary Habits' contiene valori non previsti.\")\n\n    # Codifichiamo 'Have you ever had suicidal thoughts ?': Yes=1, No=0\n    df['Have you ever had suicidal thoughts ?'] = df['Have you ever had suicidal thoughts ?'].map({'Yes': 1, 'No': 0})\n    # Controlla se ci sono valori non mappati\n    if df['Have you ever had suicidal thoughts ?'].isna().any():\n        print(\"Attenzione: Valori non previsti nella colonna 'Have you ever had suicidal thoughts ?' dopo la mappatura.\")\n\n    # Codifichiamo 'Family History of Mental Illness': Yes=1, No=0\n    df['Family History of Mental Illness'] = df['Family History of Mental Illness'].map({'Yes': 1, 'No': 0})\n    # Controlla se ci sono valori non mappati\n    if df['Family History of Mental Illness'].isna().any():\n        print(\"Attenzione: Valori non previsti nella colonna 'Family History of Mental Illness' dopo la mappatura.\")\n\n    # --- Verifica dei risultati finali ---\n    print(\"\\nDataFrame dopo TUTTE le trasformazioni (imputazione, eliminazione righe e codifica):\")\n    print(df.head())\n    print(\"\\nInformazioni sul DataFrame dopo TUTTE le trasformazioni:\")\n    print(df.info()) \n    print(\"\\nValori unici nella colonna 'Profession':\")\n    print(df['Profession'].unique())\n    print(\"Numero di valori unici:\", len(df['Profession'].unique()))\n\n    # --- Raggruppamento e Codifica 'Profession' ---\n    print(\"\\nRaggruppamento e Codifica della colonna 'Profession'...\")\n\n    # Definisci la mappatura per Profession -> Professional_Group\n    professional_map = {\n        'Chef': 'Culinary',\n        'Teacher': 'Education',\n        'Business Analyst': 'Business/Consulting',\n        'Finanancial Analyst': 'Finance',\n        'Chemist': 'Science',\n        'Electrician': 'Trades',\n        'Software Engineer': 'IT/Tech',\n        'Data Scientist': 'IT/Tech',\n        'Plumber': 'Trades',\n        'Marketing Manager': 'Marketing/Sales',\n        'Accountant': 'Finance',\n        'Entrepreneur': 'Business/Consulting',\n        'HR Manager': 'Human Resources',\n        'UX/UI Designer': 'Creative',\n        'Content Writer': 'Creative',\n        'Educational Consultant': 'Education',\n        'Civil Engineer': 'Engineering',\n        'Manager': 'Management',\n        'Pharmacist': 'Healthcare',\n        'Financial Analyst': 'Finance',\n        'Architect': 'Architecture',\n        'Mechanical Engineer': 'Engineering',\n        'Customer Support': 'Customer Service',\n        'Consultant': 'Business/Consulting',\n        'Judge': 'Legal',\n        'Researcher': 'Science',\n        'Pilot': 'Transportation',\n        'Graphic Designer': 'Creative',\n        'Travel Consultant': 'Tourism',\n        'Digital Marketer': 'Marketing/Sales',\n        'Lawyer': 'Legal',\n        'Research Analyst': 'Science',\n        'Sales Executive': 'Marketing/Sales',\n        'Doctor': 'Healthcare',\n        'Investment Banker': 'Finance',\n        'Family Consultant': 'Social Services',\n        # Le seguenti sembrano essere titoli di studio, nomi o categorie generiche.\n        # Le mappiamo a 'Other' o a gruppi pi\u00f9 specifici se appropriato.\n        'B.Com': 'Other',  # Titolo di studio, non professione\n        'BE': 'Other',     # Titolo di studio, non professione\n        'Yogesh': 'Other', # Nome\n        'Dev': 'Other',    # Nome/Categoria generica\n        'MBA': 'Other',    # Titolo di studio, non professione\n        'LLM': 'Other',    # Titolo di studio, non professione\n        'BCA': 'Other',    # Titolo di studio, non professione\n        'Academic': 'Education', # Categoria generica -> Education\n        'Profession': 'Other', # Categoria generica -> Other\n        'FamilyVirar': 'Other', # Sembra una combinazione di nome e citt\u00e0\n        'City Manager': 'Management', # Categoria specifica -> Management\n        'BBA': 'Other',    # Titolo di studio, non professione\n        'Medical Doctor': 'Healthcare', # Sinonimo di Doctor -> Healthcare\n        'MBBS': 'Other',    # Titolo di studio, non professione\n        'Patna': 'Other',  # Citt\u00e0\n        'Unveil': 'Other', # Sembra un nome/parola generica\n        'B.Ed': 'Other',    # Titolo di studio, non professione\n        'Nagpur': 'Other', # Citt\u00e0\n        'Moderate': 'Other', # Valore da Dietary Habits\n        'M.Ed': 'Other',    # Titolo di studio, non professione\n        'Analyst': 'Business/Consulting', # Categoria generica -> Business/Consulting\n        'Pranav': 'Other', # Nome\n        'Visakhapatnam': 'Other', # Citt\u00e0\n        'PhD': 'Other',    # Titolo di studio, non professione\n        'Yuvraj': 'Other'  # Nome\n    }\n\n    # Applica la mappatura e gestisci i casi non riconosciuti\n    df['Professional_Group'] = df['Profession'].map(professional_map).fillna('Other')\n\n    # --- Codifica 'Professional_Group' con LabelEncoder ---\n    print(\"\\nEseguendo Label Encoding per la colonna 'Professional_Group'...\")\n\n    # Inizializza LabelEncoder\n    label_encoder_profession = LabelEncoder()\n\n    # Utilizziamo l'encoder per 'Professional_Group'\n    df['Professional_Group_Encoded'] = label_encoder_profession.fit_transform(df['Professional_Group'])\n    print(\"\\nMappatura Label Encoding per 'Professional_Group':\")\n    # Crea una Series per mostrare la mappatura\n    label_mapping_profession = pd.Series(label_encoder_profession.classes_, name='Professional_Group').to_frame()\n    label_mapping_profession['Encoded_Value'] = label_encoder_profession.transform(label_encoder_profession.classes_)\n    print(label_mapping_profession)\n\n    # KNN per raggruppare in base alla citt\u00e0 (miglioria del metodo manuale)\n\n    # --- Definizione e Mappatura delle Regioni ---\n    # Definisci la mappatura 'region_map' per gestire tramite regione e non per City\n    region_map = {\n        # North-East India\n        'Patna': 'North-East India',\n        'Varanasi': 'North-East India',\n        # North-West India\n        'Meerut': 'North-West India', 'Ludhiana': 'North-West India',\n        'Agra': 'North-West India', 'Kanpur': 'North-West India',\n        'Jaipur': 'North-West India', 'Lucknow': 'North-West India',\n        'Srinagar': 'North-West India', 'Delhi': 'North-West India',\n        'Ghaziabad': 'North-West India', 'Faridabad': 'North-West India',\n        # West-Gujarat\n        'Surat': 'West-Gujarat', 'Vadodara': 'West-Gujarat',\n        'Rajkot': 'West-Gujarat', 'Ahmedabad': 'West-Gujarat',\n        # West-Maharashtra\n        'Kalyan': 'West-Maharashtra', 'Vasai-Virar': 'West-Maharashtra',\n        'Mumbai': 'West-Maharashtra', 'Pune': 'West-Maharashtra',\n        'Nashik': 'West-Maharashtra', 'Thane': 'West-Maharashtra',\n        # Central India\n        'Indore': 'Central India', 'Bhopal': 'Central India', 'Nagpur': 'Central India',\n        # East India\n        'Kolkata': 'East India', 'Visakhapatnam': 'East India',\n        # South India\n        'Bangalore': 'South India', 'Chennai': 'South India', 'Hyderabad': 'South India'\n    }\n\n    # Applica la mappatura e gestisci i casi non riconosciuti\n    df['Region'] = df['City'].map(region_map).fillna('Other')\n\n    # Raggruppa e conta per regione\n    print(\"\\nConteggio delle citt\u00e0 per Regione:\")\n    region_counts = df['Region'].value_counts()\n    print(region_counts)\n\n    # --- Codifica 'Region' con LabelEncoder ---\n    print(\"\\nEseguendo Label Encoding per la colonna 'Region'...\")\n\n    # Inizializza LabelEncoder\n    label_encoder = LabelEncoder()\n\n    # Fit e trasforma la colonna 'Region'\n    df['Region_Encoded'] = label_encoder.fit_transform(df['Region'])\n\n    # Puoi vedere la mappatura creata dal LabelEncoder\n    print(\"\\nMappatura Label Encoding per 'Region':\")\n    # Crea una Series per mostrare la mappatura\n    label_mapping = pd.Series(label_encoder.classes_, name='Region').to_frame()\n    label_mapping['Encoded_Value'] = label_encoder.transform(label_encoder.classes_)\n    print(label_mapping)\n\n    # --- Verifica dei risultati finali ---\n    print(\"\\nDataFrame dopo TUTTE le trasformazioni (imputazione, eliminazione righe, codifica):\")\n    print(df.head())\n    print(\"\\nInformazioni sul DataFrame dopo TUTTE le trasformazioni:\")\n    print(df.info()) # Controlla i tipi di dato e i conteggi non-null\n\n    df_clean = df.copy()\n    df_clean = df_clean.drop(columns=['City','Name','id','Profession', 'Degree', 'Degree_Group', 'Professional_Group', 'Region'])\n\n    try:\n        file_path = r'Mentally\\data\\cleaned_train.csv'\n        df_clean.to_csv(file_path, index=False)\n        print(\"\\nDataFrame esportato con successo in 'cleaned_train.csv'\")\n    except Exception as e:\n        print(f\"\\nErrore durante l'esportazione del DataFrame: {e}\")\n        \n    # # --- Visualizzazione della Matrice di Correlazione ---\n    # numeric_cols = [\n    #     'Age', 'Academic Pressure', 'Work Pressure', 'CGPA', 'Study Satisfaction',\n    #     'Job Satisfaction', 'Sleep Duration', 'Work/Study Hours', 'Financial Stress',\n    #     'Depression', 'Gender', 'Working Professional or Student', 'Dietary Habits',\n    #     'Have you ever had suicidal thoughts ?', 'Family History of Mental Illness',\n    #     'Region_Encoded', 'Degree_Group_Encoded', 'Professional_Group_Encoded'\n    # ]\n\n    # # Visualizziamo la matrice di correlazione\n    # corr_matrix = df_clean[numeric_cols].corr()\n\n    # plt.figure(figsize=(8,6))\n    # sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\")\n    # plt.title(\"Correlation matrix (variabili continue)\")\n    # plt.show()\n\n    # Modifichiamo il nome della colonna 'Have you ever had suicidal thoughts ?' in 'SuicidalThoughts' \n    df_clean = df_clean.rename(columns={'Have you ever had suicidal thoughts ?': 'SuicidalThoughts'}) \n    df_clean = df_clean.drop(columns=['SuicidalThoughts'])\n    \n    return df_clean\n\ndef preprocess_test(df):\n    # --- Preparazione del DataFrame per l'imputazione ---\n\n    # --- Imputazione della colonna 'Profession' ---\n    # Applichiamo la logica:\n    # - Se 'Working Professional or Student' \u00e8 'Student' E 'Profession' \u00e8 NaN, imputa con 'Student'.\n    # - Se 'Working Professional or Student' \u00e8 'Working Professional' E 'Profession' \u00e8 NaN, imputa con 'Unknown'.\n\n    # Imputa 'Profession' con 'Student' dove lo stato \u00e8 'Student' e 'Profession' \u00e8 mancante\n    df.loc[\n        (df['Working Professional or Student'] == 'Student') & (df['Profession'].isna()),\n        'Profession'\n    ] = 'Student'\n\n    # Imputa 'Profession' con 'Unemployed' dove lo stato \u00e8 'Working Professional' e 'Profession' \u00e8 mancante\n    df.loc[\n        (df['Working Professional or Student'] == 'Working Professional') & (df['Profession'].isna()),\n        'Profession'\n    ] = 'Unknown'\n\n    # --- Imputazione condizionale con 0 per i valori non applicabili ---\n    # Imputiamo con 0 i mancanti nelle colonne accademiche SOLO per i 'Working Professional'.\n    academic_cols = ['Academic Pressure', 'CGPA', 'Study Satisfaction']\n    for col in academic_cols:\n        # Seleziona le righe dove lo stato \u00e8 'Working Professional' e la colonna corrente \u00e8 NaN\n        df.loc[\n            (df['Working Professional or Student'] == 'Working Professional') & (df[col].isna()),\n            col\n        ] = 0\n\n    # Imputiamo con 0 i mancanti nelle colonne lavorative SOLO per gli 'Student'.\n    work_cols = ['Work Pressure', 'Job Satisfaction']\n    for col in work_cols:\n        # Seleziona le righe dove lo stato \u00e8 'Student' e la colonna corrente \u00e8 NaN\n        df.loc[\n            (df['Working Professional or Student'] == 'Student') & (df[col].isna()),\n            col\n        ] = 0\n\n    # --- Gestione dei rimanenti mancanti nelle colonne applicabili ---\n    # Questi sono i mancanti nelle colonne accademiche per gli studenti e nelle colonne lavorative per i professionisti.\n    # Li imputiamo con la mediana del rispettivo sottogruppo.\n    # Imputa i rimanenti mancanti nelle colonne accademiche per gli 'Studenti' con la mediana del sottogruppo 'Student'.\n    for col in academic_cols:\n        # Seleziona le righe dove lo stato \u00e8 'Student' e la colonna corrente \u00e8 NaN\n        condition = (df['Working Professional or Student'] == 'Student') & (df[col].isna())\n        mediana_sottogruppo = df[df['Working Professional or Student'] == 'Student'][col].median()\n        df.loc[condition, col] = mediana_sottogruppo\n\n    # Imputa i rimanenti mancanti nelle colonne lavorative per i 'Working Professional' con la mediana del sottogruppo 'Working Professional'.\n    for col in work_cols:\n        # Seleziona le righe dove lo stato \u00e8 'Working Professional' e la colonna corrente \u00e8 NaN\n        condition = (df['Working Professional or Student'] == 'Working Professional') & (df[col].isna())\n        mediana_sottogruppo = df[df['Working Professional or Student'] == 'Working Professional'][col].median()\n        df.loc[condition, col] = mediana_sottogruppo\n\n    # --- Gestione dei mancanti nelle altre colonne con pochi valori assenti ---\n    \n    # Imputazione con la moda per 'Dietary Habits' e 'Degree' (categoriche)\n    for col in ['Dietary Habits', 'Degree']:\n        # Controlla se la colonna esiste e non \u00e8 vuota prima di calcolare la moda\n        if col in df.columns and not df[col].empty:\n            # Calcola la moda solo se ci sono valori non-null\n            if df[col].notna().any():\n                moda_val = df[col].mode()[0]\n                df[col] = df[col].fillna(moda_val)\n            else:\n                print(f\"Attenzione: La colonna '{col}' contiene solo valori NaN. Imputazione con moda non possibile.\")\n\n    # Imputazione con la mediana per 'Financial Stress' (numerica)\n    if 'Financial Stress' in df.columns and not df['Financial Stress'].empty:\n        # Calcola la mediana solo se ci sono valori non-null\n        if df['Financial Stress'].notna().any():\n            mediana_val = df['Financial Stress'].median()\n            df['Financial Stress'] = df['Financial Stress'].fillna(mediana_val)\n        else:\n            print(\"Attenzione: La colonna 'Financial Stress' contiene solo valori NaN. Imputazione con mediana non possibile.\")\n\n    # --- Codifica delle variabili categoriche ---\n\n    # Codifica per 'Gender': Male=1, Female=0\n    df['Gender'] = df['Gender'].map({'Male': 1, 'Female': 0})\n    # Controlla se ci sono valori non mappati dopo la trasformazione (potrebbero diventare NaN)\n    if df['Gender'].isna().any():\n        print(\"Attenzione: Valori non previsti nella colonna 'Gender' dopo la mappatura.\")\n\n    # Codifica 'Working Professional or Student': Working Professional=1, Student=0\n    df['Working Professional or Student'] = df['Working Professional or Student'].map({'Working Professional': 1, 'Student': 0})\n    # Controlla se ci sono valori non mappati\n    if df['Working Professional or Student'].isna().any():\n        print(\"Attenzione: Valori non previsti nella colonna 'Working Professional or Student' dopo la mappatura.\")\n\n    # Applichiamo la 'map_sleep_duration' nella colonna 'Sleep Duration'\n    df['Sleep Duration'] = df['Sleep Duration'].apply(map_sleep_duration)\n\n    # Controlla se ci sono valori non mappati (dovrebbero essere NaN se la funzione non li ha riconosciuti)\n    if df['Sleep Duration'].isna().any():\n        print(\"Attenzione: Valori non previsti nella colonna 'Sleep Duration' dopo la mappatura. Potrebbero essere NaN.\")\n        mediana_sleep = df['Sleep Duration'].median()\n        df['Sleep Duration'] = df['Sleep Duration'].fillna(mediana_sleep)\n\n    # --- Pulizia e Codifica 'Degree' ---\n    # Pulizia: standardizza spazi e rimuove punteggiatura non necessaria\n    df['Degree'] = (\n        df['Degree']\n        .astype(str)\n        .str.strip()\n        .str.replace(r'\\s+', ' ', regex=True) # Standardizza spazi multipli in singolo spazio\n        .str.replace(r'[^\\w\\s.-]', '', regex=True) # Rimuove caratteri che non sono parola, spazio, punto o trattino\n        .str.title() # Mette in maiuscolo la prima lettera di ogni parola\n    )\n\n    # Mappiamo in gruppi la sezione 'Degree'\n    mapping_groups = {\n        # High School / Diploma\n        'Class 11': 'High School',\n        'Class 12': 'High School',\n        'Diploma': 'High School',\n        # Bachelor\n        'B.Tech': 'Bachelor', 'Btech': 'Bachelor',\n        'B.Sc': 'Bachelor', 'Bsc': 'Bachelor',\n        'B.Com': 'Bachelor', 'Bcom': 'Bachelor',\n        'Bca': 'Bachelor', 'Ba': 'Bachelor', 'Bba': 'Bachelor', 'Bed': 'Bachelor',\n        'B.Arch': 'Bachelor', 'Barch': 'Bachelor',\n        'B.Pharm': 'Bachelor', 'Bpharm': 'Bachelor',\n        'Bdes': 'Bachelor', 'Bfa': 'Bachelor', 'Bhm': 'Bachelor', 'Bpt': 'Bachelor',\n        'Bds': 'Bachelor', 'Bams': 'Bachelor', 'Bhms': 'Bachelor', 'Bums': 'Bachelor',\n        'B.A.': 'Bachelor', 'B.Com.': 'Bachelor', 'B.Sc.': 'Bachelor', # Aggiunti con punti\n        'B.B.A.': 'Bachelor', 'B.C.A.': 'Bachelor', 'B.Ed.': 'Bachelor',\n        # Master\n        'M.Tech': 'Master', 'Mtech': 'Master',\n        'M.Sc': 'Master', 'Msc': 'Master',\n        'M.Com': 'Master', 'Mcom': 'Master',\n        'Mca': 'Master', 'M.Ed': 'Master', 'Med': 'Master',\n        'M.Pharm': 'Master', 'Mpharm': 'Master',\n        'Mba': 'Master', 'Mdes': 'Master', 'Mfa': 'Master', 'Mhm': 'Master',\n        'Mpt': 'Master', 'Mds': 'Master', 'Mams': 'Master', 'Mhms': 'Master',\n        'M.A.': 'Master', 'M.Com.': 'Master', 'M.Sc.': 'Master', # Aggiunti con punti\n        'M.B.A.': 'Master', 'M.C.A.': 'Master', 'M.Ed.': 'Master',\n        # Doctorate / Professional\n        'Phd': 'Doctorate', 'Mbbs': 'Doctorate', 'Md': 'Doctorate', 'Llm': 'Doctorate',\n        'Ll.B.Ed': 'Doctorate', 'Ll.Ba': 'Doctorate',\n        'D.Phil': 'Doctorate', 'Dr': 'Doctorate', 'Ph.D.': 'Doctorate', # Aggiunti\n        'M.D.': 'Doctorate', 'L.L.M.': 'Doctorate',\n        # Aggiungi altri se necessario in base all'esplorazione dei dati\n    }\n\n    # Applica il mapping; i non mappati diventano 'Other'\n    df['Degree_Group'] = df['Degree'].map(mapping_groups).fillna('Other')\n\n    # --- Codifica 'Degree_Group' con OrdinalEncoder ---\n    print(\"\\nEseguendo Ordinal Encoding per la colonna 'Degree_Group'...\")\n\n    # Definisci l'ordine delle categorie per il livello di istruzione\n    # Includiamo 'Other' come livello pi\u00f9 basso per l'ordinamento.\n    degree_order = ['Other', 'High School', 'Bachelor', 'Master', 'Doctorate']\n\n    # Inizializza OrdinalEncoder con l'ordine specificato\n    ordinal_encoder_degree = OrdinalEncoder(categories=[degree_order])\n\n    # Fit e trasforma la colonna 'Degree_Group'\n    # Reshape necessario perch\u00e9 fit_transform si aspetta un input 2D\n    df['Degree_Group_Encoded'] = ordinal_encoder_degree.fit_transform(df[['Degree_Group']])\n\n    # --- Codifica 'Dietary Habits' con OrdinalEncoder ---\n    print(\"\\nValori unici nella colonna 'Dietary Habits' prima della codifica:\")\n    print(df['Dietary Habits'].unique())\n\n    # Definisci i valori validi per 'Dietary Habits'\n    valid_dietary_habits = ['Unhealthy', 'Moderate', 'Healthy']\n\n    # Elimina le righe dove 'Dietary Habits' non \u00e8 uno dei valori validi E non \u00e8 NaN\n    # Vogliamo le righe dove il valore NON \u00e8 'Unhealthy' AND NON \u00e8 'Moderate' AND NON \u00e8 'Healthy'\n    # E il valore NON \u00e8 NaN (per evitare di eliminare i NaN originali se non imputati)\n    condition_not_valid = (df['Dietary Habits'] != 'Unhealthy') & \\\n                        (df['Dietary Habits'] != 'Moderate') & \\\n                        (df['Dietary Habits'] != 'Healthy') & \\\n                        df['Dietary Habits'].notna()\n\n    # Sostituisci i valori non validi con la moda (o con 'Moderate', per esempio)\n    df.loc[condition_not_valid, 'Dietary Habits'] = df['Dietary Habits'].mode()[0]\n\n    # Ora selzioniamo gli id allineati:\n    test_ids = df['id'].values\n\n    print(f\"Numero di righe rimanenti nel DataFrame: {len(df)}\")\n\n    # Ristampa i valori unici dopo l'eliminazione per verifica\n    print(\"\\nValori unici nella colonna 'Dietary Habits' dopo l'eliminazione delle righe non valide:\")\n    print(df['Dietary Habits'].unique())\n\n    # Ordine: Unhealthy < Moderate < Healthy\n    encoder = OrdinalEncoder(categories=[valid_dietary_habits])\n    # Reshape necessario perch\u00e9 fit_transform si aspetta un input 2D\n\n    try:\n        # Modifica per usare solo i valori validi definiti\n        encoder = OrdinalEncoder(categories=[valid_dietary_habits])\n        df['Dietary Habits'] = encoder.fit_transform(df[['Dietary Habits']])\n        print(\"\\nCodifica 'Dietary Habits' completata con successo.\")\n        \n    except ValueError as e:\n        print(f\"\\nErrore durante la codifica di 'Dietary Habits': {e}\")\n        print(\"Controlla i valori unici stampati sopra. La colonna 'Dietary Habits' contiene valori non previsti.\")\n\n    # Codifica 'Have you ever had suicidal thoughts ?': Yes=1, No=0\n    df['Have you ever had suicidal thoughts ?'] = df['Have you ever had suicidal thoughts ?'].map({'Yes': 1, 'No': 0})\n    # Controlla se ci sono valori non mappati\n    if df['Have you ever had suicidal thoughts ?'].isna().any():\n        print(\"Attenzione: Valori non previsti nella colonna 'Have you ever had suicidal thoughts ?' dopo la mappatura.\")\n\n    # Codifica 'Family History of Mental Illness': Yes=1, No=0\n    df['Family History of Mental Illness'] = df['Family History of Mental Illness'].map({'Yes': 1, 'No': 0})\n    # Controlla se ci sono valori non mappati\n    if df['Family History of Mental Illness'].isna().any():\n        print(\"Attenzione: Valori non previsti nella colonna 'Family History of Mental Illness' dopo la mappatura.\")\n\n    # --- Verifica dei risultati finali ---\n    print(\"\\nDataFrame dopo TUTTE le trasformazioni (imputazione, eliminazione righe e codifica):\")\n    print(df.head())\n    print(\"\\nInformazioni sul DataFrame dopo TUTTE le trasformazioni:\")\n    print(df.info())\n    print(\"\\nValori unici nella colonna 'Profession':\")\n    print(df['Profession'].unique())\n    print(\"Numero di valori unici:\", len(df['Profession'].unique()))\n\n    # --- Raggruppamento e Codifica 'Profession' ---\n    print(\"\\nRaggruppamento e Codifica della colonna 'Profession'...\")\n\n    # Definisco la mappatura per Professional_Group in base a Profession\n    professional_map = {\n        'Chef': 'Culinary',\n        'Teacher': 'Education',\n        'Business Analyst': 'Business/Consulting',\n        'Finanancial Analyst': 'Finance',\n        'Chemist': 'Science',\n        'Electrician': 'Trades',\n        'Software Engineer': 'IT/Tech',\n        'Data Scientist': 'IT/Tech',\n        'Plumber': 'Trades',\n        'Marketing Manager': 'Marketing/Sales',\n        'Accountant': 'Finance',\n        'Entrepreneur': 'Business/Consulting',\n        'HR Manager': 'Human Resources',\n        'UX/UI Designer': 'Creative',\n        'Content Writer': 'Creative',\n        'Educational Consultant': 'Education',\n        'Civil Engineer': 'Engineering',\n        'Manager': 'Management',\n        'Pharmacist': 'Healthcare',\n        'Financial Analyst': 'Finance',\n        'Architect': 'Architecture',\n        'Mechanical Engineer': 'Engineering',\n        'Customer Support': 'Customer Service',\n        'Consultant': 'Business/Consulting',\n        'Judge': 'Legal',\n        'Researcher': 'Science',\n        'Pilot': 'Transportation',\n        'Graphic Designer': 'Creative',\n        'Travel Consultant': 'Tourism',\n        'Digital Marketer': 'Marketing/Sales',\n        'Lawyer': 'Legal',\n        'Research Analyst': 'Science',\n        'Sales Executive': 'Marketing/Sales',\n        'Doctor': 'Healthcare',\n        'Investment Banker': 'Finance',\n        'Family Consultant': 'Social Services',\n        # Le seguenti sembrano essere titoli di studio, nomi o categorie generiche.\n        # Le mappiamo a 'Other' o a gruppi pi\u00f9 specifici se appropriato.\n        'B.Com': 'Other', # Titolo di studio, non professione\n        'BE': 'Other',    # Titolo di studio, non professione\n        'Yogesh': 'Other', # Nome\n        'Dev': 'Other',    # Nome/Categoria generica\n        'MBA': 'Other',    # Titolo di studio, non professione\n        'LLM': 'Other',    # Titolo di studio, non professione\n        'BCA': 'Other',    # Titolo di studio, non professione\n        'Academic': 'Education', # Categoria generica -> Education\n        'Profession': 'Other', # Categoria generica -> Other\n        'FamilyVirar': 'Other', # Sembra una combinazione di nome e citt\u00e0\n        'City Manager': 'Management', # Categoria specifica -> Management\n        'BBA': 'Other',    # Titolo di studio, non professione\n        'Medical Doctor': 'Healthcare', # Sinonimo di Doctor -> Healthcare\n        'MBBS': 'Other',    # Titolo di studio, non professione\n        'Patna': 'Other',  # Citt\u00e0\n        'Unveil': 'Other', # Sembra un nome/parola generica\n        'B.Ed': 'Other',    # Titolo di studio, non professione\n        'Nagpur': 'Other', # Citt\u00e0\n        'Moderate': 'Other', # Valore da Dietary Habits\n        'M.Ed': 'Other',    # Titolo di studio, non professione\n        'Analyst': 'Business/Consulting', # Categoria generica -> Business/Consulting\n        'Pranav': 'Other', # Nome\n        'Visakhapatnam': 'Other', # Citt\u00e0\n        'PhD': 'Other',    # Titolo di studio, non professione\n        'Yuvraj': 'Other'  # Nome\n    }\n\n    # Applichiamo la mappatura e gestisci i casi non riconosciuti\n    df['Professional_Group'] = df['Profession'].map(professional_map).fillna('Other')\n\n    # --- Codifica 'Professional_Group' con LabelEncoder ---\n    print(\"\\nEseguendo Label Encoding per la colonna 'Professional_Group'...\")\n\n    # Inizializziamo LabelEncoder\n    label_encoder_profession = LabelEncoder()\n\n    # Fit e trasforma la colonna 'Professional_Group'\n    df['Professional_Group_Encoded'] = label_encoder_profession.fit_transform(df['Professional_Group'])\n\n    # Puoi vedere la mappatura creata dal LabelEncoder\n    print(\"\\nMappatura Label Encoding per 'Professional_Group':\")\n    # Crea una Series per mostrare la mappatura\n    label_mapping_profession = pd.Series(label_encoder_profession.classes_, name='Professional_Group').to_frame()\n    label_mapping_profession['Encoded_Value'] = label_encoder_profession.transform(label_encoder_profession.classes_)\n    print(label_mapping_profession)\n    \n    # KNN per raggruppare in base alla citt\u00e0 (miglioria del metodo manuale)\n\n    # --- Definizione e Mappatura delle Regioni ---\n    # Definiamo la mappatura Region per City\n    region_map = {\n        # North-East India\n        'Patna': 'North-East India',\n        'Varanasi': 'North-East India',\n        # North-West India\n        'Meerut': 'North-West India', 'Ludhiana': 'North-West India',\n        'Agra': 'North-West India', 'Kanpur': 'North-West India',\n        'Jaipur': 'North-West India', 'Lucknow': 'North-West India',\n        'Srinagar': 'North-West India', 'Delhi': 'North-West India',\n        'Ghaziabad': 'North-West India', 'Faridabad': 'North-West India',\n        # West-Gujarat\n        'Surat': 'West-Gujarat', 'Vadodara': 'West-Gujarat',\n        'Rajkot': 'West-Gujarat', 'Ahmedabad': 'West-Gujarat',\n        # West-Maharashtra\n        'Kalyan': 'West-Maharashtra', 'Vasai-Virar': 'West-Maharashtra',\n        'Mumbai': 'West-Maharashtra', 'Pune': 'West-Maharashtra',\n        'Nashik': 'West-Maharashtra', 'Thane': 'West-Maharashtra',\n        # Central India (unchanged)\n        'Indore': 'Central India', 'Bhopal': 'Central India', 'Nagpur': 'Central India',\n        # East India (unchanged)\n        'Kolkata': 'East India', 'Visakhapatnam': 'East India',\n        # South India (unchanged)\n        'Bangalore': 'South India', 'Chennai': 'South India', 'Hyderabad': 'South India'\n    }\n\n    # Applica la mappatura e gestisci i casi non riconosciuti\n    df['Region'] = df['City'].map(region_map).fillna('Other')\n\n    # Raggruppa e conta per regione\n    print(\"\\nConteggio delle citt\u00e0 per Regione:\")\n    region_counts = df['Region'].value_counts()\n    print(region_counts)\n\n    # --- Codifica 'Region' con LabelEncoder ---\n    print(\"\\nEseguendo Label Encoding per la colonna 'Region'...\")\n\n    # Inizializziamo LabelEncoder\n    label_encoder = LabelEncoder()\n\n    # Fit e trasforma la colonna 'Region'\n    df['Region_Encoded'] = label_encoder.fit_transform(df['Region'])\n    print(\"\\nMappatura Label Encoding per 'Region':\")\n    # Crea una Series per mostrare la mappatura\n    label_mapping = pd.Series(label_encoder.classes_, name='Region').to_frame()\n    label_mapping['Encoded_Value'] = label_encoder.transform(label_encoder.classes_)\n    print(label_mapping)\n\n    # --- Verifica dei risultati finali ---\n    print(\"\\nDataFrame dopo TUTTE le trasformazioni (imputazione, eliminazione righe, codifica):\")\n    print(df.head())\n    print(\"\\nInformazioni sul DataFrame dopo TUTTE le trasformazioni:\")\n    print(df.info())\n   \n    df_clean = df.copy()\n    df_clean = df_clean.drop(columns=['City','Name','id','Profession', 'Degree', 'Degree_Group', 'Professional_Group', 'Region'])\n\n    try:\n        file_path = r'Mentally\\data\\cleaned_test.csv'\n        df_clean.to_csv(file_path, index=False)\n        print(\"\\nDataFrame esportato con successo in 'cleaned_test.csv'\")\n    except Exception as e:\n        print(f\"\\nErrore durante l'esportazione del DataFrame: {e}\")\n\n    # # --- Visualizzazione della Matrice di Correlazione ---\n    # numeric_cols = [\n    #     'Age', 'Academic Pressure', 'Work Pressure', 'CGPA', 'Study Satisfaction',\n    #     'Job Satisfaction', 'Sleep Duration', 'Work/Study Hours', 'Financial Stress',\n    #     'Gender', 'Working Professional or Student', 'Dietary Habits',\n    #     'Have you ever had suicidal thoughts ?', 'Family History of Mental Illness',\n    #     'Region_Encoded', 'Degree_Group_Encoded', 'Professional_Group_Encoded'\n    # ]\n\n    # # Calcolo la Pearson-corr\n    # corr_matrix = df_clean[numeric_cols].corr()\n\n    # plt.figure(figsize=(8,6))\n    # sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\")\n    # plt.title(\"Correlation matrix (variabili continue)\")\n    # plt.show()\n\n    df_clean = df_clean.rename(columns={'Have you ever had suicidal thoughts ?': 'SuicidalThoughts'}) \n    df_clean = df_clean.drop(columns=['SuicidalThoughts'])\n    \n    return df_clean, test_ids\n\ndef preprocess_person_test(df):\n    # --- Preparazione del DataFrame per l'imputazione ---\n\n    # --- Imputazione della colonna 'Profession' ---\n    # Applichiamo la logica:\n    # - Se 'Working Professional or Student' \u00e8 'Student' E 'Profession' \u00e8 NaN, imputa con 'Student'.\n    # - Se 'Working Professional or Student' \u00e8 'Working Professional' E 'Profession' \u00e8 NaN, imputa con 'Unknown'.\n\n    # Imputa 'Profession' con 'Student' dove lo stato \u00e8 'Student' e 'Profession' \u00e8 mancante\n    df.loc[\n        (df['Working Professional or Student'] == 'Student') & (df['Profession'].isna()),\n        'Profession'\n    ] = 'Student'\n\n    # Imputa 'Profession' con 'Unemployed' dove lo stato \u00e8 'Working Professional' e 'Profession' \u00e8 mancante\n    df.loc[\n        (df['Working Professional or Student'] == 'Working Professional') & (df['Profession'].isna()),\n        'Profession'\n    ] = 'Unknown'\n\n    # --- Imputazione condizionale con 0 per i valori non applicabili ---\n    # Imputiamo con 0 i mancanti nelle colonne accademiche SOLO per i 'Working Professional'.\n    academic_cols = ['Academic Pressure', 'CGPA', 'Study Satisfaction']\n    for col in academic_cols:\n        # Seleziona le righe dove lo stato \u00e8 'Working Professional' e la colonna corrente \u00e8 NaN\n        df.loc[\n            (df['Working Professional or Student'] == 'Working Professional') & (df[col].isna()),\n            col\n        ] = 0\n\n    # Imputiamo con 0 i mancanti nelle colonne lavorative SOLO per gli 'Student'.\n    work_cols = ['Work Pressure', 'Job Satisfaction']\n    for col in work_cols:\n        # Seleziona le righe dove lo stato \u00e8 'Student' e la colonna corrente \u00e8 NaN\n        df.loc[\n            (df['Working Professional or Student'] == 'Student') & (df[col].isna()),\n            col\n        ] = 0\n\n    # --- Gestione dei rimanenti mancanti nelle colonne applicabili ---\n    # Questi sono i mancanti nelle colonne accademiche per gli studenti e nelle colonne lavorative per i professionisti.\n    # Li imputiamo con la mediana del rispettivo sottogruppo.\n    # Imputa i rimanenti mancanti nelle colonne accademiche per gli 'Studenti' con la mediana del sottogruppo 'Student'.\n    for col in academic_cols:\n        # Seleziona le righe dove lo stato \u00e8 'Student' e la colonna corrente \u00e8 NaN\n        condition = (df['Working Professional or Student'] == 'Student') & (df[col].isna())\n        mediana_sottogruppo = df[df['Working Professional or Student'] == 'Student'][col].median()\n        df.loc[condition, col] = mediana_sottogruppo\n\n    # Imputa i rimanenti mancanti nelle colonne lavorative per i 'Working Professional' con la mediana del sottogruppo 'Working Professional'.\n    for col in work_cols:\n        # Seleziona le righe dove lo stato \u00e8 'Working Professional' e la colonna corrente \u00e8 NaN\n        condition = (df['Working Professional or Student'] == 'Working Professional') & (df[col].isna())\n        mediana_sottogruppo = df[df['Working Professional or Student'] == 'Working Professional'][col].median()\n        df.loc[condition, col] = mediana_sottogruppo\n\n    # --- Gestione dei mancanti nelle altre colonne con pochi valori assenti ---\n    \n    # Imputazione con la moda per 'Dietary Habits' e 'Degree' (categoriche)\n    for col in ['Dietary Habits', 'Degree']:\n        # Controlla se la colonna esiste e non \u00e8 vuota prima di calcolare la moda\n        if col in df.columns and not df[col].empty:\n            # Calcola la moda solo se ci sono valori non-null\n            if df[col].notna().any():\n                moda_val = df[col].mode()[0]\n                df[col] = df[col].fillna(moda_val)\n            else:\n                print(f\"Attenzione: La colonna '{col}' contiene solo valori NaN. Imputazione con moda non possibile.\")\n\n    # Imputazione con la mediana per 'Financial Stress' (numerica)\n    if 'Financial Stress' in df.columns and not df['Financial Stress'].empty:\n        # Calcola la mediana solo se ci sono valori non-null\n        if df['Financial Stress'].notna().any():\n            mediana_val = df['Financial Stress'].median()\n            df['Financial Stress'] = df['Financial Stress'].fillna(mediana_val)\n        else:\n            print(\"Attenzione: La colonna 'Financial Stress' contiene solo valori NaN. Imputazione con mediana non possibile.\")\n\n    # --- Codifica delle variabili categoriche ---\n\n    # Codifica per 'Gender': Male=1, Female=0\n    df['Gender'] = df['Gender'].map({'Male': 1, 'Female': 0})\n    # Controlla se ci sono valori non mappati dopo la trasformazione (potrebbero diventare NaN)\n    if df['Gender'].isna().any():\n        print(\"Attenzione: Valori non previsti nella colonna 'Gender' dopo la mappatura.\")\n\n    # Codifica 'Working Professional or Student': Working Professional=1, Student=0\n    df['Working Professional or Student'] = df['Working Professional or Student'].map({'Working Professional': 1, 'Student': 0})\n    # Controlla se ci sono valori non mappati\n    if df['Working Professional or Student'].isna().any():\n        print(\"Attenzione: Valori non previsti nella colonna 'Working Professional or Student' dopo la mappatura.\")\n\n    # Applichiamo la 'map_sleep_duration' nella colonna 'Sleep Duration'\n    df['Sleep Duration'] = df['Sleep Duration'].apply(map_sleep_duration)\n\n    # Controlla se ci sono valori non mappati (dovrebbero essere NaN se la funzione non li ha riconosciuti)\n    if df['Sleep Duration'].isna().any():\n        print(\"Attenzione: Valori non previsti nella colonna 'Sleep Duration' dopo la mappatura. Potrebbero essere NaN.\")\n        mediana_sleep = df['Sleep Duration'].median()\n        df['Sleep Duration'] = df['Sleep Duration'].fillna(mediana_sleep)\n\n    # --- Pulizia e Codifica 'Degree' ---\n    # Pulizia: standardizza spazi e rimuove punteggiatura non necessaria\n    df['Degree'] = (\n        df['Degree']\n        .astype(str)\n        .str.strip()\n        .str.replace(r'\\s+', ' ', regex=True) # Standardizza spazi multipli in singolo spazio\n        .str.replace(r'[^\\w\\s.-]', '', regex=True) # Rimuove caratteri che non sono parola, spazio, punto o trattino\n        .str.title() # Mette in maiuscolo la prima lettera di ogni parola\n    )\n\n    # Mappiamo in gruppi la sezione 'Degree'\n    mapping_groups = {\n        # High School / Diploma\n        'Class 11': 'High School',\n        'Class 12': 'High School',\n        'Diploma': 'High School',\n        # Bachelor\n        'B.Tech': 'Bachelor', 'Btech': 'Bachelor',\n        'B.Sc': 'Bachelor', 'Bsc': 'Bachelor',\n        'B.Com': 'Bachelor', 'Bcom': 'Bachelor',\n        'Bca': 'Bachelor', 'Ba': 'Bachelor', 'Bba': 'Bachelor', 'Bed': 'Bachelor',\n        'B.Arch': 'Bachelor', 'Barch': 'Bachelor',\n        'B.Pharm': 'Bachelor', 'Bpharm': 'Bachelor',\n        'Bdes': 'Bachelor', 'Bfa': 'Bachelor', 'Bhm': 'Bachelor', 'Bpt': 'Bachelor',\n        'Bds': 'Bachelor', 'Bams': 'Bachelor', 'Bhms': 'Bachelor', 'Bums': 'Bachelor',\n        'B.A.': 'Bachelor', 'B.Com.': 'Bachelor', 'B.Sc.': 'Bachelor', # Aggiunti con punti\n        'B.B.A.': 'Bachelor', 'B.C.A.': 'Bachelor', 'B.Ed.': 'Bachelor',\n        # Master\n        'M.Tech': 'Master', 'Mtech': 'Master',\n        'M.Sc': 'Master', 'Msc': 'Master',\n        'M.Com': 'Master', 'Mcom': 'Master',\n        'Mca': 'Master', 'M.Ed': 'Master', 'Med': 'Master',\n        'M.Pharm': 'Master', 'Mpharm': 'Master',\n        'Mba': 'Master', 'Mdes': 'Master', 'Mfa': 'Master', 'Mhm': 'Master',\n        'Mpt': 'Master', 'Mds': 'Master', 'Mams': 'Master', 'Mhms': 'Master',\n        'M.A.': 'Master', 'M.Com.': 'Master', 'M.Sc.': 'Master', # Aggiunti con punti\n        'M.B.A.': 'Master', 'M.C.A.': 'Master', 'M.Ed.': 'Master',\n        # Doctorate / Professional\n        'Phd': 'Doctorate', 'Mbbs': 'Doctorate', 'Md': 'Doctorate', 'Llm': 'Doctorate',\n        'Ll.B.Ed': 'Doctorate', 'Ll.Ba': 'Doctorate',\n        'D.Phil': 'Doctorate', 'Dr': 'Doctorate', 'Ph.D.': 'Doctorate', # Aggiunti\n        'M.D.': 'Doctorate', 'L.L.M.': 'Doctorate',\n        # Aggiungi altri se necessario in base all'esplorazione dei dati\n    }\n\n    # Applica il mapping; i non mappati diventano 'Other'\n    df['Degree_Group'] = df['Degree'].map(mapping_groups).fillna('Other')\n\n    # --- Codifica 'Degree_Group' con OrdinalEncoder ---\n    print(\"\\nEseguendo Ordinal Encoding per la colonna 'Degree_Group'...\")\n\n    # Definisci l'ordine delle categorie per il livello di istruzione\n    # Includiamo 'Other' come livello pi\u00f9 basso per l'ordinamento.\n    degree_order = ['Other', 'High School', 'Bachelor', 'Master', 'Doctorate']\n\n    # Inizializza OrdinalEncoder con l'ordine specificato\n    ordinal_encoder_degree = OrdinalEncoder(categories=[degree_order])\n\n    # Fit e trasforma la colonna 'Degree_Group'\n    # Reshape necessario perch\u00e9 fit_transform si aspetta un input 2D\n    df['Degree_Group_Encoded'] = ordinal_encoder_degree.fit_transform(df[['Degree_Group']])\n\n    # --- Codifica 'Dietary Habits' con OrdinalEncoder ---\n    print(\"\\nValori unici nella colonna 'Dietary Habits' prima della codifica:\")\n    print(df['Dietary Habits'].unique())\n\n    # Definisci i valori validi per 'Dietary Habits'\n    valid_dietary_habits = ['Unhealthy', 'Moderate', 'Healthy']\n\n    # Elimina le righe dove 'Dietary Habits' non \u00e8 uno dei valori validi E non \u00e8 NaN\n    # Vogliamo le righe dove il valore NON \u00e8 'Unhealthy' AND NON \u00e8 'Moderate' AND NON \u00e8 'Healthy'\n    # E il valore NON \u00e8 NaN (per evitare di eliminare i NaN originali se non imputati)\n    condition_not_valid = (df['Dietary Habits'] != 'Unhealthy') & \\\n                        (df['Dietary Habits'] != 'Moderate') & \\\n                        (df['Dietary Habits'] != 'Healthy') & \\\n                        df['Dietary Habits'].notna()\n\n    # Sostituisci i valori non validi con la moda (o con 'Moderate', per esempio)\n    df.loc[condition_not_valid, 'Dietary Habits'] = df['Dietary Habits'].mode()[0]\n\n    # Ora selzioniamo gli id allineati:\n    test_ids = df['id'].values\n\n    print(f\"Numero di righe rimanenti nel DataFrame: {len(df)}\")\n\n    # Ristampa i valori unici dopo l'eliminazione per verifica\n    print(\"\\nValori unici nella colonna 'Dietary Habits' dopo l'eliminazione delle righe non valide:\")\n    print(df['Dietary Habits'].unique())\n\n    # Ordine: Unhealthy < Moderate < Healthy\n    encoder = OrdinalEncoder(categories=[valid_dietary_habits])\n    # Reshape necessario perch\u00e9 fit_transform si aspetta un input 2D\n\n    try:\n        # Modifica per usare solo i valori validi definiti\n        encoder = OrdinalEncoder(categories=[valid_dietary_habits])\n        df['Dietary Habits'] = encoder.fit_transform(df[['Dietary Habits']])\n        print(\"\\nCodifica 'Dietary Habits' completata con successo.\")\n        \n    except ValueError as e:\n        print(f\"\\nErrore durante la codifica di 'Dietary Habits': {e}\")\n        print(\"Controlla i valori unici stampati sopra. La colonna 'Dietary Habits' contiene valori non previsti.\")\n\n    # Codifica 'Have you ever had suicidal thoughts ?': Yes=1, No=0\n    df['Have you ever had suicidal thoughts ?'] = df['Have you ever had suicidal thoughts ?'].map({'Yes': 1, 'No': 0})\n    # Controlla se ci sono valori non mappati\n    if df['Have you ever had suicidal thoughts ?'].isna().any():\n        print(\"Attenzione: Valori non previsti nella colonna 'Have you ever had suicidal thoughts ?' dopo la mappatura.\")\n\n    # Codifica 'Family History of Mental Illness': Yes=1, No=0\n    df['Family History of Mental Illness'] = df['Family History of Mental Illness'].map({'Yes': 1, 'No': 0})\n    # Controlla se ci sono valori non mappati\n    if df['Family History of Mental Illness'].isna().any():\n        print(\"Attenzione: Valori non previsti nella colonna 'Family History of Mental Illness' dopo la mappatura.\")\n\n    # --- Verifica dei risultati finali ---\n    print(\"\\nDataFrame dopo TUTTE le trasformazioni (imputazione, eliminazione righe e codifica):\")\n    print(df.head())\n    print(\"\\nInformazioni sul DataFrame dopo TUTTE le trasformazioni:\")\n    print(df.info())\n    print(\"\\nValori unici nella colonna 'Profession':\")\n    print(df['Profession'].unique())\n    print(\"Numero di valori unici:\", len(df['Profession'].unique()))\n\n    # --- Raggruppamento e Codifica 'Profession' ---\n    print(\"\\nRaggruppamento e Codifica della colonna 'Profession'...\")\n\n    # Definisco la mappatura per Professional_Group in base a Profession\n    professional_map = {\n        'Chef': 'Culinary',\n        'Teacher': 'Education',\n        'Business Analyst': 'Business/Consulting',\n        'Finanancial Analyst': 'Finance',\n        'Chemist': 'Science',\n        'Electrician': 'Trades',\n        'Software Engineer': 'IT/Tech',\n        'Data Scientist': 'IT/Tech',\n        'Plumber': 'Trades',\n        'Marketing Manager': 'Marketing/Sales',\n        'Accountant': 'Finance',\n        'Entrepreneur': 'Business/Consulting',\n        'HR Manager': 'Human Resources',\n        'UX/UI Designer': 'Creative',\n        'Content Writer': 'Creative',\n        'Educational Consultant': 'Education',\n        'Civil Engineer': 'Engineering',\n        'Manager': 'Management',\n        'Pharmacist': 'Healthcare',\n        'Financial Analyst': 'Finance',\n        'Architect': 'Architecture',\n        'Mechanical Engineer': 'Engineering',\n        'Customer Support': 'Customer Service',\n        'Consultant': 'Business/Consulting',\n        'Judge': 'Legal',\n        'Researcher': 'Science',\n        'Pilot': 'Transportation',\n        'Graphic Designer': 'Creative',\n        'Travel Consultant': 'Tourism',\n        'Digital Marketer': 'Marketing/Sales',\n        'Lawyer': 'Legal',\n        'Research Analyst': 'Science',\n        'Sales Executive': 'Marketing/Sales',\n        'Doctor': 'Healthcare',\n        'Investment Banker': 'Finance',\n        'Family Consultant': 'Social Services',\n        # Le seguenti sembrano essere titoli di studio, nomi o categorie generiche.\n        # Le mappiamo a 'Other' o a gruppi pi\u00f9 specifici se appropriato.\n        'B.Com': 'Other', # Titolo di studio, non professione\n        'BE': 'Other',    # Titolo di studio, non professione\n        'Yogesh': 'Other', # Nome\n        'Dev': 'Other',    # Nome/Categoria generica\n        'MBA': 'Other',    # Titolo di studio, non professione\n        'LLM': 'Other',    # Titolo di studio, non professione\n        'BCA': 'Other',    # Titolo di studio, non professione\n        'Academic': 'Education', # Categoria generica -> Education\n        'Profession': 'Other', # Categoria generica -> Other\n        'FamilyVirar': 'Other', # Sembra una combinazione di nome e citt\u00e0\n        'City Manager': 'Management', # Categoria specifica -> Management\n        'BBA': 'Other',    # Titolo di studio, non professione\n        'Medical Doctor': 'Healthcare', # Sinonimo di Doctor -> Healthcare\n        'MBBS': 'Other',    # Titolo di studio, non professione\n        'Patna': 'Other',  # Citt\u00e0\n        'Unveil': 'Other', # Sembra un nome/parola generica\n        'B.Ed': 'Other',    # Titolo di studio, non professione\n        'Nagpur': 'Other', # Citt\u00e0\n        'Moderate': 'Other', # Valore da Dietary Habits\n        'M.Ed': 'Other',    # Titolo di studio, non professione\n        'Analyst': 'Business/Consulting', # Categoria generica -> Business/Consulting\n        'Pranav': 'Other', # Nome\n        'Visakhapatnam': 'Other', # Citt\u00e0\n        'PhD': 'Other',    # Titolo di studio, non professione\n        'Yuvraj': 'Other'  # Nome\n    }\n\n    # Applichiamo la mappatura e gestisci i casi non riconosciuti\n    df['Professional_Group'] = df['Profession'].map(professional_map).fillna('Other')\n\n    # --- Codifica 'Professional_Group' con LabelEncoder ---\n    print(\"\\nEseguendo Label Encoding per la colonna 'Professional_Group'...\")\n\n    # Inizializziamo LabelEncoder\n    label_encoder_profession = LabelEncoder()\n\n    # Fit e trasforma la colonna 'Professional_Group'\n    df['Professional_Group_Encoded'] = label_encoder_profession.fit_transform(df['Professional_Group'])\n\n    # Puoi vedere la mappatura creata dal LabelEncoder\n    print(\"\\nMappatura Label Encoding per 'Professional_Group':\")\n    # Crea una Series per mostrare la mappatura\n    label_mapping_profession = pd.Series(label_encoder_profession.classes_, name='Professional_Group').to_frame()\n    label_mapping_profession['Encoded_Value'] = label_encoder_profession.transform(label_encoder_profession.classes_)\n    print(label_mapping_profession)\n    \n    # KNN per raggruppare in base alla citt\u00e0 (miglioria del metodo manuale)\n\n    # --- Definizione e Mappatura delle Regioni ---\n    # Definiamo la mappatura Region per City\n    region_map = {\n        # North-East India\n        'Patna': 'North-East India',\n        'Varanasi': 'North-East India',\n        # North-West India\n        'Meerut': 'North-West India', 'Ludhiana': 'North-West India',\n        'Agra': 'North-West India', 'Kanpur': 'North-West India',\n        'Jaipur': 'North-West India', 'Lucknow': 'North-West India',\n        'Srinagar': 'North-West India', 'Delhi': 'North-West India',\n        'Ghaziabad': 'North-West India', 'Faridabad': 'North-West India',\n        # West-Gujarat\n        'Surat': 'West-Gujarat', 'Vadodara': 'West-Gujarat',\n        'Rajkot': 'West-Gujarat', 'Ahmedabad': 'West-Gujarat',\n        # West-Maharashtra\n        'Kalyan': 'West-Maharashtra', 'Vasai-Virar': 'West-Maharashtra',\n        'Mumbai': 'West-Maharashtra', 'Pune': 'West-Maharashtra',\n        'Nashik': 'West-Maharashtra', 'Thane': 'West-Maharashtra',\n        # Central India (unchanged)\n        'Indore': 'Central India', 'Bhopal': 'Central India', 'Nagpur': 'Central India',\n        # East India (unchanged)\n        'Kolkata': 'East India', 'Visakhapatnam': 'East India',\n        # South India (unchanged)\n        'Bangalore': 'South India', 'Chennai': 'South India', 'Hyderabad': 'South India'\n    }\n\n    # Applica la mappatura e gestisci i casi non riconosciuti\n    df['Region'] = df['City'].map(region_map).fillna('Other')\n\n    # Raggruppa e conta per regione\n    print(\"\\nConteggio delle citt\u00e0 per Regione:\")\n    region_counts = df['Region'].value_counts()\n    print(region_counts)\n\n    # --- Codifica 'Region' con LabelEncoder ---\n    print(\"\\nEseguendo Label Encoding per la colonna 'Region'...\")\n\n    # Inizializziamo LabelEncoder\n    label_encoder = LabelEncoder()\n\n    # Fit e trasforma la colonna 'Region'\n    df['Region_Encoded'] = label_encoder.fit_transform(df['Region'])\n    print(\"\\nMappatura Label Encoding per 'Region':\")\n    # Crea una Series per mostrare la mappatura\n    label_mapping = pd.Series(label_encoder.classes_, name='Region').to_frame()\n    label_mapping['Encoded_Value'] = label_encoder.transform(label_encoder.classes_)\n    print(label_mapping)\n\n    # --- Verifica dei risultati finali ---\n    print(\"\\nDataFrame dopo TUTTE le trasformazioni (imputazione, eliminazione righe, codifica):\")\n    print(df.head())\n    print(\"\\nInformazioni sul DataFrame dopo TUTTE le trasformazioni:\")\n    print(df.info())\n   \n    df_clean = df.copy()\n    df_clean = df_clean.drop(columns=['City','Name','id','Profession', 'Degree', 'Degree_Group', 'Professional_Group', 'Region'])\n\n    try:\n        file_path = r'Mentally\\data\\person_test.csv'\n        df_clean.to_csv(file_path, index=False)\n        print(\"\\nDataFrame esportato con successo in 'person_test.csv'\")\n    except Exception as e:\n        print(f\"\\nErrore durante l'esportazione del DataFrame: {e}\")\n\n    df_clean = df_clean.rename(columns={'Have you ever had suicidal thoughts ?': 'SuicidalThoughts'}) \n    df_clean = df_clean.drop(columns=['SuicidalThoughts'])\n    \n    return df_clean"
    }
]